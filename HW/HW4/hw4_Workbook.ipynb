{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"2f5235b9-a2b4-4fd3-a148-7e8fc65676c1","showTitle":false,"title":""}},"source":["## HW 4 - Supervised Learning at Scale.\n","__`MIDS w261: Machine Learning at Scale | UC Berkeley School of Information | Spring 2023`__\n","\n","> Updated: 06/18/2023\n","\n","HW4 is composed of two sections (see module 6 for access to the HW4 sections):\n","\n","* Section 1: A timed quiz (20 questions) with all questions available in Digital Campus only. You will need a Spark cluster to complete the questions. You will get three attempts to complete this quiz and you will get some feedback after each attempt. \n","* Section 2: A regular assignment in the form of a Jupyter notebook and Digital Campus quiz with no immediate feedback upon submission. \n","NOTE: both parts will be graded and will contribute to your overall grade for the semester.\n","\n","This notebook and corresponding Digital Campus quiz forms part 2 of 2 for HW4.\n","\n","In the first three HW assignment, you became familiar with the Map-Reduce programming paradigm as manifested in the Hadoop Streaming and Spark frameworks. We explored how different data structures and design patterns can help us manage the computational complexity of an algorithm. As part of this process you implemented both a supervised learning alogorithm (Naive Bayes) and an unsupervised learning algorithm (synonym detection via cosine similarity). In both of these tasks parallelization helped us manage calculations involving a large number of features. However a large feature space isn't the only situation that might prompt us to want to parallelize a machine learning algorithm. In the final two assignments we'll look at cases where the iterative nature of an algorithm is the main driver of its computational complexity (and the reason we might want to parallelize it).\n","\n","In this assignment, you will perform learn linear models using different loss functions: OLS, RidgeMSE, LassoMSE, BXE, CXE, RidgeBXE, LASSOBXE, etc.. As in previous assignments, you will implement the core calculations using Spark RDDs, and you will also start working with Spark DataFrames. Though we've provided more of a code base than before since the focus of the latter half of the course is more on general machine learning concepts. By the end of this homework you should be able to:\n","\n","* ... __define__ the loss functions for linear regression and linear classifier models: OLS MSE, RidgeMSE, LassoMSE, BXE, CXE, RidgeBXE, LASSOBXE, etc. \n","* ... __calculate__ the gradient for each of these loss functions.\n","* ... __identify__ which parts of the gradient descent algorithm can be parallelized.\n","* ... __implement__ parallelized gradient descent with cross-validation and regularization.\n","* ... __compare/contrast__ how L1 and L2 regularization impact model parameters & performance.\n","\n","Additional Reference: [latest (3.2.0) Spark Documentation - RDD programming guide](https://spark.apache.org/docs/3.2.0/rdd-programming-guide.html)\n","\n","__Please refer to the `README` for homework submission instructions and additional resources.__"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"9799fd7a-8875-4ee2-aa5a-1de4b1f99173","showTitle":false,"title":""}},"source":["## Notebook Set-Up\n","Before starting your homework run the following cells to confirm your setup."]},{"cell_type":"markdown","metadata":{},"source":["### Data and HW output vs notebook Locations: private data bucket versus dataproc staging bucket\n","\n","When you create a DataProc cluster, HDFS is used as the default filesystem. In this course, we override this behavior by setting the defaultFS as a Cloud Storage bucket. The benefits of using Cloud Storage buckets are that your job results get to persist beyond the lifetime of the cluster (and btw latency on these cloud buckets is super low).\n","\n","In this HW, you use your personal cloud bucket (and folders on them), known as DATA_BUCKET, as:\n","* input and output for the Spark Apps that you will develop as part of your submission.  \n","* you will save your jupyter notebooks and data on your personal cloud bucket also!\n","\n","The datasets for this homework will be downloaded and saved into your `private Data Bucket` on Google Cloud. Recall that you created a private data bucket during the first week of semester. You may have called it w261-<your initials>. Jimi's bucket name is `w261-jgs`. To facilitate ease of access, we have set up location variables for the course-related data buckets. Your private data bucket  can be accessed via: \n","\n","* ```python\n","    import os\n","    DATA_BUCKET = os.getenv('DATA_BUCKET','')[:-1] # our private storage bucket location\n","```\n","\n","This DATA_BUCKET will be used for hosting the input data for this assignment and also to store output from your HW Assignment apps.\n","Associated with each DataProc cluster is a persistant storage bucket that we refer to as the DataProc Staging Bucket. You will be using this staging bucket to store notebooks and other files associated with your HW assignments, and  live sessions. The location of the staging bucket  is made available via `os.getenv(\"STAGING_BUCKET\")`. Since this bucket is persistent, we will no longer need to snapshot your student workspaces. \n","    \n","For more background on Dataproc staging  buckets please see:\n","* [https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Access your personal data bucket and see whats there (RUN THIS CELL AS IS)\n","import os\n","DATA_BUCKET = os.getenv('DATA_BUCKET','')[:-1] # our private storage bucket location\n","HW4_FOLDER = f\"{DATA_BUCKET}/notebooks/jupyter/Assignments/HW4\"\n","print(f\"Personal Data bucket:  {DATA_BUCKET}\")\n","!gsutil ls -lh  {HW4_FOLDER}  #lets have a look the HW folder on our private storage bucket"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"336dd213-f583-4fbe-9c90-cb86ba100fd1","showTitle":false,"title":""}},"outputs":[],"source":["# imports\n","import re\n","import time\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import ast\n","import os"]},{"cell_type":"markdown","metadata":{},"source":["### Start Spark and the Spark UI"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# to shutdown Spark. (RUN THIS CELL AS IS)\n","# if Spark is not running no problem\n","# if Spark is running no problem also it will be shutdown\n","\n","try:\n","    spark\n","    print(f\"{sc.master} appName: {sc.appName}\")\n","    spark.sparkContext.stop()\n","    del spark; del sc\n","    print(f\"Just Shutdown Spark.\")\n","except NameError:\n","    print(\"Spark is NOT currently running\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"336dd213-f583-4fbe-9c90-cb86ba100fd1","showTitle":false,"title":""}},"outputs":[],"source":["# start Spark Session (RUN THIS CELL AS IS)\n","#Step A: Start Spark by running the following cell\n","\n","from pyspark.sql import SparkSession\n","\n","try:\n","    spark\n","    print(\"Spark is already running\")\n","    print(f\"{sc.master} appName: {sc.appName}\")\n","except NameError:\n","    print('starting Spark')\n","    app_name = 'hw4_notebook'\n","    master = \"local[*]\"\n","    spark = SparkSession\\\n","            .builder\\\n","            .appName(app_name)\\\n","            .master(master)\\\n","            .getOrCreate()\n","sc = spark.sparkContext\n","\n","# Don't worry about messages shown below"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# get Spark Session info (RUN THIS CELL AS IS)\n","spark"]},{"cell_type":"markdown","metadata":{},"source":["### OPTIONAL:  to view the Spark UI (jobs and stages) \n","Web UI (aka Application UI or webUI or Spark UI) is the web interface of a running Spark application to monitor and inspect Spark job executions in a web browser. The following is a screenshot of the Spark UI. Feel free to skip this section.  \n","\n","<img src='https://www.dropbox.com/s/rithkjbboymr0ey/SparkUI_screenshot.png?raw=true' style='width:80%'>\n","\n","\n","Getting access to Spark UI, you will need to create a tunnel from DataProc via the CloudShell. Please follow all three steps depicted below (starting with running the following code cell):\n","\n","* Launch Spark first (as described above)!\n","* The run the following command (make sure your specify the correct zone!\" \n","   * \n","``` bash\n","   gcloud compute ssh w261-m --zone us-central1-a --ssh-flag \"-L 8080:localhost:4040\"\n","   ```"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Optional to view the Spark UI (RUN THIS CELL AS IS)\n","ZONE = os.getenv('ZONE','') # zone where the DataProc cluster is running\n","ui_port = spark._repr_html_().split(\".internal:\")[-1].split('\"')[0]\n","print(ZONE)\n","print(\"Copy the following command (and swap out the ZONE for your cluster zone) to cloud shell\\n\"+ \n","      \"and run as shown in the screenshot below. \\n\"+\n","      \"And click on the PREVIEW ON PORT 8080 menu option in the web preview menu (look for the icon <>).\\n\\n\")\n","f'   gcloud compute ssh w261-m --zone {ZONE} --ssh-flag \"-L 8080:localhost:4040\"    '"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# gcloud compute ssh w261-m --zone us-central1-a --ssh-flag \"-L 8080:localhost:4040\""]},{"cell_type":"markdown","metadata":{},"source":["<img src='https://www.dropbox.com/s/tlb4uiakj2bx7qg/Three_steps_launch_SparkUI.png?raw=true' style='width:100%'>"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"b7b227ac-5c1e-4399-9e8b-e0106dcc9c75","showTitle":false,"title":""}},"source":["# Question 1: Opimization Theory \n","\n","As you know from w207, Gradient Descent is an iterative process that seeks to find the optimal parameters for a model given a particular training data set. It does this by using the vector of partial derivatives of a loss function to strategically update parameters in a way that will reduce the loss. In live session 6, you discussed some of the theory behind why gradient descent works and looked at a small example of gradient descent in the context of linear regression.\n","\n","### Q1 Tasks:\n","\n","* __a) Multiple Dropdowns on Digital Campus:__ What are the first and second order conditions for convexity and why do we care about them when performing Gradient Descent?\n","\n","* __b) Multiple Dropdowns on Digital Campus:__ Explain the relationship between problem domain space and model parameter space in the context of Gradient Descent. In practice, why can't we find the optimal model by simply looking at the error surface in model parameter space?\n","\n","* __c) Matching on Digital Campus:__ In the context of Gradient Descent, what is the 'learning rate' and what are the tradeoffs associated with setting this hyperparameter?\n","\n","* __d) Multiple Choice on Digital Campus:__ In the context of OLS, what do we mean by a 'closed-form solution' and is it scalable?"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"6472d5e7-29a1-420f-ba74-bd855e049de2","showTitle":false,"title":""}},"source":["### Q1 Student Answers:\n","OPTIONAL - if you'd like to save your answers in this notebook: \n","\n","> __a)__ Type your answer here!\n","\n","> __b)__ Type your answer here!\n","\n","> __c)__ Type your answer here!  \n","\n","> __d)__ Type your answer here!"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"53c7548b-a503-49cc-826d-6519e1408b10","showTitle":false,"title":""}},"source":["# Load data and do EDA\n","\n","## About the Data\n","\n","For the main task in this portion of the homework you will use data about red and white Portuguese wines. [This data](http://archive.ics.uci.edu/ml/datasets/Wine+Quality) was made available to the UC Irvine public repository of Machine Learning datasets by researchers at the University of Minho in association with [this paper](https://www.sciencedirect.com/science/article/pii/S0167923609001377?via%3Dihub):\n","> P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. \n","Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.\n","\n","The dataset includes 12 fields:\n",">`fixed acidity`  \n","`volatile acidity`  \n","`citric acid`  \n","`residual sugar`  \n","`chlorides`  \n","`free sulfur dioxide`  \n","`total sulfur dioxide`  \n","`density`  \n","`pH`  \n","`sulphates`  \n","`alcohol`  \n","`quality`   -- (_a score between 0 and 10_): TARGET Variable\n","\n","__`IMPORTANT NOTE:`__ The outcome variable in our data is a human assigned `quality `score ranging from 0 to 10. Since the scores are integers this is actually an ordinal and not numerical outcome varaible. However for the purposes of this assignment we'll treat it as a numerical quantity.\n","\n","The data are in two files: one containing red wines and another containing white wines.  Use the following cells to download the data, add a field for red/white, and split it into a test and train set."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"0bd4f45e-b0ab-4a11-8b75-5ed7ecfda13f","showTitle":false,"title":""}},"outputs":[],"source":["# make a data folder & download red & white wine files\n","!mkdir data\n","!wget -q -O data/reds.csv http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\n","!wget -q -O data/whites.csv http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!ls -l data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!head data/reds.csv"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["HW4_FOLDER"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# put data on personal bucket - RUN THIS CELL AS IS\n","data_loc = f'{HW4_FOLDER}/data/reds.csv'\n","!cat data/reds.csv | gsutil cp - {data_loc}   \n","data_loc = f'{HW4_FOLDER}/data/whites.csv'\n","!cat data/whites.csv | gsutil cp - {data_loc}                                    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!gsutil ls -l {HW4_FOLDER}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# load data - RUN THIS CELL AS IS\n","data_loc = f'{HW4_FOLDER}/data/whites.csv'\n","!gsutil cat  -h  {data_loc}|head"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"a01b6070-6862-4152-a7c9-8da61b79d84c","showTitle":false,"title":""}},"outputs":[],"source":["# extract header fields - RUN THIS CELL AS IS\n","header = !head -n 1 data/reds.csv\n","header = header[0]\n","FIELDS = ['color'] + re.sub('\"', '', header).split(';')"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"70cc1772-2983-40bf-a64c-77140a164817","showTitle":false,"title":""}},"outputs":[],"source":["# load the raw data into an RDD - RUN THIS CELL AS IS\n","data_loc = f'{HW4_FOLDER}/data/reds.csv'\n","redsRDD = sc.textFile(data_loc)\\\n","            .filter(lambda x: x != header)\\\n","            .map(lambda x: '1;' + x) # set first field 1 to indicate red wine\n","data_loc = f'{HW4_FOLDER}/data/whites.csv'\n","whitesRDD = sc.textFile(data_loc)\\\n","              .filter(lambda x: x != header)\\\n","              .map(lambda x: '0;' + x) # set first field 0 to indicate white wine\n","redsRDD.count(), whitesRDD.count()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"a4275958-4cf3-4806-a9f3-d4a7b859c20b","showTitle":false,"title":""}},"outputs":[],"source":["redsRDD.take(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"4ec5e621-0fc0-4e8b-8cf9-ce6476201e25","showTitle":false,"title":""}},"outputs":[],"source":["# Generate 80/20 (pseudo)random train/test split - RUN THIS CELL AS IS\n","trainRDD, heldOutRDD = redsRDD.union(whitesRDD).randomSplit([0.8,0.2], seed=1)\n","print(f\"... held out {heldOutRDD.count()} records for evaluation and assigned {trainRDD.count()} for training.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"eaf4de86-27f1-405c-8388-90b1c9e69b30","showTitle":false,"title":""}},"outputs":[],"source":["import pandas as pd\n","pd.DataFrame(trainRDD.map(lambda line: line.split(';')).take(5), columns=[\"color\", \"fixed acidity\",\"volatile acidity\",\"citric acid\",\n","                                        \"residual sugar\",\"chlorides\",\"free sulfur dioxide\",\n","                                        \"total sulfur dioxide\",\"density\",\"pH\",\"sulphates\",\"alcohol\",\"quality\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"6c0da7af-2c2e-4c6d-97f3-10c4720079bb","showTitle":false,"title":""}},"outputs":[],"source":["# helper function - RUN THIS CELL AS IS\n","def parse(line):\n","    \"\"\"\n","    Map record_csv_string --> (tuple,of,fields)\n","    \"\"\"\n","    fields = np.array(line.split(';'), dtype = 'float')\n","    features,quality = fields[:-1], fields[-1]\n","    return(features, quality)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"d9445952-22a9-4759-a10d-48b07345b55c","showTitle":false,"title":""}},"outputs":[],"source":["# cache the training set - RUN THIS CELL AS IS \n","trainRDDCached = trainRDD.map(parse).cache()"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"fc3912d0-f86d-4229-be9d-b384c121fbe9","showTitle":false,"title":""}},"source":["## Question 2: EDA\n","\n","A statistician's approach to Linear Regression typically involves a series of EDA steps to examine each feature in the data and then a series of steps to test assumptions about their potential contribution to a multi-feature linear model. In particular, we'd want to look for a set of features that exhibit a likely linear relationship with the outcome variable and that are _not_ highy correlated with each other. In the context of machine learning, these considerations remain important techniques for improving model generalizability despite the common practice to use model evaluation techniques (and large data sets) to get the final word on feature selection. \n","\n","In this question we'll briefly look at the features in our data set. To mimic an 'at scale' analysis we'll start by sampling from our Spark RDD training set so that we have a manageable amount of data to work with in our visuals.\n","\n","Here, we detour briefly and explore different methods of normality tests based on graphical methods and statistical methods.\n","\n","#### Visual Normality Checks: historgrams and Q-Q plot\n","We can create plots of the data to check whether the each feature is Gaussian distributed. These checks are qualitative, so less accurate than the statistical methods we will calculate in the next section. Nevertheless, they are fast and like the statistical tests, must still be interpreted before you can make a call about your data sample. A simple and commonly used plot to quickly check the distribution of a sample of data is the histogram. A histogram can be created using the hist() matplotlib function. By default, the number of bins is automatically estimated from the data sample. If a variable's data, summarized using histrogram-based form, takes a bell-shape corresponding to  Gaussian-like shape, then we can qualitatively say it is is a normally-distributed variable. Another popular plot for checking the distribution of a data sample is the quantile-quantile plot, `Q-Q plot`, or QQ plot for short.\n","\n","In this section, we will highlighted two common methods for visually inspecting a dataset to check if it was drawn from a Gaussian distribution.\n","\n","#### Statistical Normality Tests\n","There are many statistical tests that we can use to quantify whether a sample of data looks as though it was drawn from a Gaussian distribution. Each test makes different assumptions and considers different aspects of the data.\n","\n","We will list commonly used tests that one can apply to ones own data samples:\n","\n","* Shapiro-Wilk test\n","* D’Agostino’s $K^2$ Test\n","* Anderson-Darling Test\n","\n","We explore the Shapiro-Wilk test in detail. Please feel free to use any of these tests to check for normality of variable in a programmatic way. The Shapiro-Wilk test evaluates a data sample and quantifies how likely it is that the data was drawn from a Gaussian distribution, named for Samuel Shapiro and Martin Wilk. In practice, the Shapiro-Wilk test is believed to be a reliable test of normality, although there is some suggestion that the test may be suitable for smaller samples of data, e.g. thousands of observations or fewer.\n","\n","The Shapiro-Wilk is a hypotheses test and the two hypotheses are as follows:\n","\n","* $H_o$(Accepted): Sample is from the normal distributions.(Po>0.05)\n","* $H_a$(Rejected): Sample is not from the normal distributions.\n","\n","The `scipy.stats.shapiro()` SciPy function calculates the Shapiro-Wilk on a given dataset. The function returns both the W-statistic calculated by the test and the p-value. The complete example of performing the Shapiro-Wilk test on the dataset is given here:."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Shapiro-Wilk Test\n","from numpy.random import seed\n","from numpy.random import randn\n","from scipy.stats import shapiro\n","# seed the random number generator\n","seed(1)\n","# generate univariate observations\n","data = 5 * randn(100) + 50\n","# normality test\n","stat, p = shapiro(data)\n","print('Statistics=%.3f, p=%.3f' % (stat, p))\n","# interpret\n","alpha = 0.05\n","if p > alpha:\n","    print('Sample looks Gaussian (fail to reject H0)')\n","else:\n","    print('Sample does not look Gaussian (reject H0)')"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"fc3912d0-f86d-4229-be9d-b384c121fbe9","showTitle":false,"title":""}},"source":["### Q2 Tasks:\n","\n","* __a) Multiple Answers on Digital Campus:__ Run the provided code to sample 1000 points and visualize histograms of each feature. Which features appear normally distributed? (Select 2 answers)\n","\n","* __b) Multiple Choice on Digital Campus:__ How is the varaible `color` different than the other features & what does that mean about how we interpret its regression coefficient?\n","\n","* __c) Multiple Answers on Digital Campus:__ Run the provided code to create boxplots of each feature. Which, if any, appear to have a **positive** linear relationship with `quality`? (Select 1 answer)\n","\n","* __d) Multiple Answers on Digital Campus:__ Which if any appear to have a **negative** linear relationship with `quality`? (Select 2 answers)\n","\n","* __e) Multiple Answers on Digital Campus:__ Run the provided code to plot the correlations matrix. Which pairs of features are most _strongly_ (**postively**) associated with each other? (Select 2 answers)\n","\n","* __f) Multiple Answers on Digital Campus:__ Which pairs of features are most _strongly_ (**negatively**) associated with each other? (Select 2 answers)\n","\n","* __g) Multiple Choice on Digital Campus:__ What implications would that have for our feature selection?\n","\n","    * A:  We would want to remove all combinations of these features that are highly correlated. For example, before modeling we should remove the following features from the training data: color, total sulfur dioxide and free sulfur dioxide combination.\n","    * B:  We would want to remove some combinations of these features in feature selection so they do not interfere with each other's predictive power. Some features to remove could be color and free sulfur dioxide, since they both correlate strongly with total sulfur dioxide.\n","    * C:  We should build a linear regression model using all available input features. Since multicollinearity is only important for a statistical model and does not affect a machine learning model.\n","    * D: None of the provided options is correct.\n"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"e91f5427-d635-406a-a3ab-bcdca13d16bd","showTitle":false,"title":""}},"source":["### Q2 Student Answers:\n","\n","OPTIONAL - if you'd like to save your answers in this notebook: \n","\n","> __a)__ Type your answer here!  \n","\n","> __b)__ Type your answer here!  \n","\n","> __c)__ Type your answer here!\n","\n","> __d)__ Type your answer here!  \n","\n","> __e)__ Type your answer here!  \n","\n","> __f)__ Type your answer here!\n","\n","> __g)__ Type your answer here!"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"8c048484-8fd0-499f-98a0-7e631e2d544b","showTitle":false,"title":""}},"outputs":[],"source":["# part a - take a 1000 point sample for EDA (RUN THIS CELL AS IS)\n","sample = np.array(trainRDDCached.map(lambda x: np.append(x[0], [x[1]]))\n","                                .takeSample(False, 1000, seed=2022))\n","sample_df = pd.DataFrame(np.array(sample), columns = FIELDS)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"be0343d0-70f9-46ca-8a06-dc9e76ed155c","showTitle":false,"title":""}},"outputs":[],"source":["# part a1 - take a look at histograms for each feature (RUN THIS CELL AS IS)\n","sample_df[FIELDS[:-1]].hist(figsize=(15,15), bins=15)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# part a2 - take a look at the Shapiro-Wilk Statistical Normality Tests (RUN THIS CELL AS IS)\n","# Shapiro-Wilk Test\n","from numpy.random import seed\n","from numpy.random import randn\n","from scipy.stats import shapiro\n","# interpret\n","alpha = 0.05\n","for i in sample_df:\n","    stat, p = shapiro(sample_df[[i]])\n","    #print(sample_df[[i]])\n","    if p > alpha:\n","        print(f'Variable {i} looks Gaussian (fail to reject H0) at {p:0.5}')\n","    else:\n","        print(f'Variable {i} does NOT look Gaussian (reject H0)  at {p:.5}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# part b -  plot boxplots of each feature vs. the outcome (RUN THIS CELL AS IS)\n","fig, ax_grid = plt.subplots(4, 3, figsize=(15,15))\n","y = sample_df['quality']\n","for idx, feature in enumerate(FIELDS[:-1]):\n","    x = sample_df[feature]\n","    sns.boxplot(x=x, y=y, ax=ax_grid[idx//3][idx%3], orient='h', linewidth=.5)\n","    ax_grid[idx//3][idx%3].invert_yaxis()\n","fig.suptitle(\"Individual Features vs. Outcome (qualilty)\", fontsize=15, y=0.9)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"10d7c4c2-2f27-425e-9a20-4f1cd73303d1","showTitle":false,"title":""}},"outputs":[],"source":["# plot heatmap for correlations matrix - RUN THIS CELL AS IS\n","corr = sample_df[FIELDS[:-1]].corr()\n","fig, ax = plt.subplots(figsize=(11, 9))\n","mask = np.zeros_like(corr, dtype=np.bool)\n","mask[np.triu_indices_from(mask)] = True\n","cmap = sns.diverging_palette(240, 10, as_cmap=True)\n","sns.heatmap(corr, mask=mask, cmap=cmap, center=0, linewidths=.5)\n","plt.title(\"Correlations between features.\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"da631eaf-d3db-4f74-b94e-580d2cf4817a","showTitle":false,"title":""}},"source":["# Question 3: OLS Loss\n","\n","For a parametric model, the key factor that will impact how easy it is to optimize is your choice of how to define the loss function. In Ordinary Least Squares (OLS) Regression our loss function is just about as convenient as you will get: not only is it convex, its also very easy to interpret. \n","\n","When doing supervised learning, a simple sanity check consists of comparing one’s estimator against simple rules of thumb. It is useful as a simple baseline to compare with other (real) regressors. Examples of regression baselines include:\n","* \"mean\": always predicts the mean of the training set\n","* \"median\": always predicts the median of the training set\n","* \"quantile\": always predicts a specified quantile of the training set,provided with the quantile parameter.\n","* \"constant\": always predicts a constant value that is provided by the user.\n","\n","In this question you'll \"choose\" a baseline model and then write a function to compute the loss of a linear model in Spark. You'll reuse this function in Q4 when you implement gradient descent.\n","\n","#### Baseline example illustrated:"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"f5942c8d-87c5-4fe4-a9bd-035e7caaf02c","showTitle":false,"title":""}},"outputs":[],"source":["# points from our mini example from the demo 6 notebook\n","points = np.array([[1,2],[3,4],[5,5],[4,3],[2,3]])\n","x = points[:,0]\n","y = points[:,1]\n","\n","plt.figure()\n","plt.plot(x, y,'o', label='data points')\n","plt.axhline(np.mean(y),c='r', label='\"mean\" model')\n","plt.title('Example of \"mean\" baseline model')\n","plt.ylabel(\"y\")\n","plt.xlabel(\"x\")\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"27d79c61-c4eb-4ddf-838c-59f3e10ff840","showTitle":false,"title":""}},"source":["### Q3 Tasks:\n","* __a) code + numeric answer on Digital Campus:__ Fill in the code below to compute the mean of your outcome variable. [__`HINT:`__ _use `trainRDDCached` as the input & feel free to use Spark built-in functions._]\n","\n","* __b) code + numeric answer on Digital Campus:__ Fill in the code below to compute the variance of your outcome variable. [__`HINT:`__ _use `trainRDDCached` as the input & feel free to use Spark built-in functions._]\n","\n","* __c) Multiple Choice on Digital Campus:__ Which is the correct formula and interpretation for the OLS loss function?\n","\n","* __d) Multiple Choice on Digital Campus:__ In the context of linear models & vector computations what does it mean to 'augment' a datapoint and why do we do this?\n","\n","* __e) code:__ Fill in the missing code to complete the`OLSLoss` function. [__`TIP:`__ Use `augmentedData` as your input when computing the loss.]\n","\n","* __f) Multiple Choice on Digital Campus:__ Is computing the loss \"embarassingly parallel\"?\n","\n","* __g) code + numeric answer on Digital Campus:__ Fill in the missing code to define a baseline model for this data set that has a bias term equal to the mean of your outcome variable and `0.0` for all coefficients. Note that in the docstring for `OLSLoss` we specified that the model should be a numpy array with the bias in the first position. Once you've defined your model, run the provided cells to check that your model has the correct dimensions and then compute the loss for your baseline model.\n","\n","* __h) Multiple Choice on Digital Campus:__  Compare your results in part __g__ to the result you got in part __a__. What is the explanation?"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"1d514045-6cc7-4dfd-a3ff-ab581277cddb","showTitle":false,"title":""}},"source":["### Q3 Student Answers:\n","\n","OPTIONAL - if you'd like to save your answers in this notebook: \n","\n","> __a)__ Type your answer here!  \n","\n","> __b)__ Type your answer here!  \n","\n","> __c)__ Type your answer here!\n","\n","> __d)__ Type your answer here!  \n","\n","> __f)__ Type your answer here!\n","\n","> __g)__ Type your answer here!\n","\n","> __h)__ Type your answer here!  "]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"9617c461-d2e9-44f5-8637-2d06880e3de6","showTitle":false,"title":""}},"outputs":[],"source":["# part a - mean of the outcome variable \n","meanQuality = None   # FILL IN YOUR CODE HERE\n","print(f\"Mean: {meanQuality}\")\n","# EXPECTED OUTPUT: 5.811040339702759"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# part b - variance of the outcome variable \n","varQuality = None   # FILL IN YOUR CODE HERE\n","print(f\"Variance: {varQuality}\")\n","# EXPECTED OUTPUT: 0.760086564865664"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"94d09af5-bb09-4cdd-bcbe-0c0f0a621a39","showTitle":false,"title":""}},"outputs":[],"source":["# part e - write function to compute loss (FILL IN MISSING CODE BELOW)\n","def OLSLoss(dataRDD, W):\n","    \"\"\"\n","    Compute mean squared error.\n","    Args:\n","        dataRDD - each record is a tuple of (features_array, y)\n","        W       - (array) model coefficients with bias at index 0\n","    \"\"\"\n","    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[0]), x[1]))\n","    ################## YOUR CODE HERE ##################\n","    loss = None\n","    ################## (END) YOUR CODE ##################\n","    return loss"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"14982bbd-8436-4a3a-971c-32a1f7287cd4","showTitle":false,"title":""}},"outputs":[],"source":["# part g - define your baseline model here\n","BASELINE = None   # FILL IN YOUR CODE HERE"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"4d315379-a192-4d6a-99fa-230a554e9081","showTitle":false,"title":""}},"outputs":[],"source":["# part g - compute the loss for your baseline model (RUN THIS CELL AS IS)\n","assert len(BASELINE) == len(trainRDDCached.take(1)[0][0]) + 1, \"Double check model dimensions\"\n","print(f\"Baseline model loss: {OLSLoss(trainRDDCached, BASELINE)}\")\n","# EXPECTED OUTPUT: 0.7600865648656636"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"345a836b-ae3f-4270-a302-53a9d63dcc0d","showTitle":false,"title":""}},"source":["# Question 4: Vanilla Gradient Descent\n","\n","Performing Gradient Descent technically only requires two steps: 1) _use the current model to calculate the gradient_; 2) _use the gradient to update the current model parameters_. In practice though, we'll want to add a third step which is to compute the loss for our new model so that we can see if its working. In this question you'll implement gradient descent for OLS regression and take a look at a few update steps.\n","\n","### Q4 Tasks:\n","* __a) Multiple Choice on Digital Campus:__ Select the correct statements regarding the augmentation of data in terms of machine learning\n","\n","* __b) Short Essay on Digital Campus:__ The size of the update for gradient descent is influenced by two factors:\n","    * The learning rate. Let's assume it is a constant here as therefore can be dropped from further discussion with regard to this question.\n","    * Gradient vector:\n","      * In OLS Regression the gradient vector is composed of two terms:\n","        * The training data\n","        * The error term\n","      * In plain language, the gradient vector is  **the weighted average of the training data where each example is weighted by its associated error**.\n","      * To keep things simple, assume we are using all the training data to calculate the gradient vector.\n","\n","    __TASK__: As part of your response, please provide the formula for the gradient (__in Latex__) to reflect this plain language summary. Then explain under what circumstances you would have a large gradient and large subsequent update, and under what circumstances you would have a small gradient and small subsequent update.\n","    \n","    (Hint: discuss in terms of those two core terms)\n","\n","* __c) Matching on Digital Campus:__ Examine the formula you wrote in question Q4(a2). With a gradient descent implementation in Spark Map-Reduce framework, for learning a linear regression model as the context, please match each Left Hand Side question with the most appropriate Right Hand Side response.\n","\n","* __d) Fill in Blanks on Digital Campus:__ The mean squared error (MSE) loss function, which is a metric used to evaluate a simple Linear Regression model is defined as the following: \n","$$\\text{MSE}(\\mathbf{w_1,w_0}; \\mathbf{X,y}) = \\dfrac{1}{m} \\sum\\limits_{i=1}^{m}{( \\mathbf{w_1} \\mathbf{x}^{(i)}+ \\mathbf{w_0} - y^{(i)})^2}$$\n","    where,\n","    $$X = \\begin{bmatrix}\n","    - \\ \\mathbf x_1^T - \\\\ \n","      \\vdots  \\\\ \n","    - \\ \\mathbf x_m^T -   \\\\ \n","    \\end{bmatrix} = \\begin{bmatrix}\n","    x_{11} &; \\cdots &; x_{1n}  \\\\ \n","     &;  \\ddots &;  \\\\ \n","    x_{m1} &; \\cdots &; x_{mn}  \\\\ \n","    \\end{bmatrix}$$\n","    $m$ traning examples $(x_i, y_i)$\n","    \n","    $n$ features, $\\mathbf x_i = \\big[x_{i1}, \\ ... \\ , x_{in} \\big]^T \\in \\mathbb{R}^n$\n","    \n","    the observed target values $\\mathbf y = \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_m\\end{bmatrix} \\in \\mathbb{R}^{m}$\n","    \n","    With respect to Linear Regression Loss function, in Coordinate Geometry, the model coefficient $w_1$ is more commonly known as the `[value1]`, while $w_0$ is more commonly known as the `[value2]`.  Please answer the `[value 1]` and `[value 2]` in the blank. (Expect a single word for `[value1]`, `[value2]`)\n","\n","* __e) Code:__ Fill in the missing lines in `GDUpdate` to compute the gradient and perform a single update of the model parameters.   \n","    * __`TIP 1:`__ _remember that the gradient is a vector of partial derivatives, `grad` should be a numpy array_    \n","    * __`TIP 2:`__ _Spark's built in `mean()` function may help you here_  \n","\n","\n","* __f) Multiple Choice on Digital Campus:__ Run the provided code to perform 5 steps of Gradient Descent on our data. What is wrong with these results?\n","\n","* __g) Code:__ Fill in the missing code in `normalize` so that this function scales each feature and centers it at 0. Then use the provide code block to rerun your same gradient descent code on the scaled data. \n","\n","* __h) Multiple Choice on Digital Campus:__ Use these results to explain what the problem was in question '4f'.\n"," "]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"b6644459-75c7-4b20-bd8c-c043d8286532","showTitle":false,"title":""}},"source":["### Q4 Student Answers:\n","\n","OPTIONAL - if you'd like to save your answers in this notebook: \n","\n","> __a)__ Type your answer here!  \n","\n","> __b)__ Type your answer here!  \n","\n","> __c)__ Type your answer here! \n","\n","> __d)__ Type your answer here! \n","\n","> __e)__ complete the coding portions of this question before answering f\n","\n","> __f)__ Type your answer here!\n","\n","> __g)__ complete the coding portions of this question before answering h \n","\n","> __h)__ Type your answer here!\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"5c7f11a8-c529-4fe1-9b94-9138cdff9ad0","showTitle":false,"title":""}},"outputs":[],"source":["# part e - function to perform a single GD step\n","def GDUpdate(dataRDD, W, learningRate = 0.1):\n","    \"\"\"\n","    Perform one OLS gradient descent step/update.\n","    Args:\n","        dataRDD - records are tuples of (features_array, y)\n","        W       - (array) model coefficients with bias at index 0\n","    Returns:\n","        new_model - (array) updated coefficients, bias at index 0\n","    \"\"\"\n","    # add a bias 'feature' of 1 at index 0\n","    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[0]), x[1])).cache()\n","    \n","    ################## YOUR CODE HERE ################# \n","    grad = None\n","    new_model = None\n","    ################## (END) YOUR CODE ################# \n","   \n","    return new_model"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"b76eba8f-cfe0-4d77-9d78-ba64357b69cf","showTitle":false,"title":""}},"outputs":[],"source":["%%time\n","# part f - take a look at a few Gradient Descent steps (RUN THIS CELL AS IS)\n","\n","nSteps = 5\n","model = BASELINE\n","print(f\"BASELINE:  Loss = {OLSLoss(trainRDDCached,model)}\")\n","for idx in range(nSteps):\n","    print(\"----------\")\n","    print(f\"STEP: {idx+1}\")\n","    model = GDUpdate(trainRDDCached, model)\n","    loss = OLSLoss(trainRDDCached, model)\n","    print(f\"Loss: {loss}\")\n","    print(f\"Model: {[round(w,3) for w in model]}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<b> Question 4F Expected Output <b>\n","\n","\n","<img src='https://github.com/UCB-w261/w261-environment/blob/master/hw-images/HW4%20Expected%20Outputs/Q4F%20Expected%20Output.png?raw=true' style='width:80%'>"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"bebf2cfd-ecbe-4d74-bba5-23eaf95f5ded","showTitle":false,"title":""}},"outputs":[],"source":["# part g - helper function to normalize the data (FILL IN THE MISSING CODE BELOW)\n","def normalize(dataRDD):\n","    \"\"\"\n","    Scale and center data round mean of each feature.\n","    Args:\n","        dataRDD - records are tuples of (features_array, y)\n","    Returns:\n","        normedRDD - records are tuples of (features_array, y)\n","    \"\"\"\n","    featureMeans = dataRDD.map(lambda x: x[0]).mean()\n","    featureStdev = np.sqrt(dataRDD.map(lambda x: x[0]).variance())\n","    \n","    ################ YOUR CODE HERE #############\n","    normedRDD = None\n","    ################ FILL IN YOUR CODE HERE #############\n","    \n","    return normedRDD"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"4cb99ab5-511f-4cd3-9803-5c37871d89ce","showTitle":false,"title":""}},"outputs":[],"source":["# part g - cache normalized data (RUN THIS CELL AS IS)\n","normedRDD = normalize(trainRDDCached).cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"ba2a42bc-46b4-4a63-8ccd-9637e6f3bf24","showTitle":false,"title":""}},"outputs":[],"source":["%%time\n","# part g - take a look at a few GD steps w/ normalized data  (RUN THIS CELL AS IS)\n","nSteps = 5\n","model = BASELINE\n","print(f\"BASELINE:  Loss = {OLSLoss(trainRDDCached,model)}\")\n","for idx in range(nSteps):\n","    print(\"----------\")\n","    print(f\"STEP: {idx+1}\")\n","    model = GDUpdate(normedRDD, model)\n","    loss = OLSLoss(normedRDD, model) \n","    print(f\"Loss: {loss}\")\n","    print(f\"Model: {[round(w,3) for w in model]}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<b> Question 4G Expected Output <b>\n","\n","\n","<img src='https://github.com/UCB-w261/w261-environment/blob/master/hw-images/HW4%20Expected%20Outputs/Q4G%20Expected%20Output.png?raw=true' style='width:80%'>"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"f65a4eb5-0a92-4154-9e7f-f1ba436319df","showTitle":false,"title":""}},"source":["# Question 5: Assessing the performance of your model.\n","\n","Printing out the loss as we perform each gradient descent step allows us to confirm that our Gradient Descent code appears to be working, but this number doesn't accurately reflect \"how good\" our model is. In this question, you'll plot error curves for a test set and training set in order to discuss the model performance. Note that although we have split out a test set & train set when we first loaded the data, in the spirit of keeping that 20% test set truly 'held out' until the end of the assignment, we'll make an additional split for the purposes of this question, and divide the existing training set into two smaller RDDs.\n","\n","### Q5 Tasks:\n","\n","* __a) Multiple Choice on Digital Campus:__ Why doesn't the loss that we printed in Question 4 accurately reflect \"how good\" our model is? \n","\n","\n","* __b) Code:__ Since we're going to be running Gradient Descent for a number of times, let's package it into a function. Fill in the missing code in the `GradientDescent()` function. (Feel free to use the provided code blocks as a starting point.)\n","\n","\n","* __c) Multiple Choice on Digital Campus:__ Use the provided code to split the normalized data into a test set and train set, then run 50 iterations of gradient descent and plot the MSE curves for each. Describe what you see and speculate about why this might be happening.\n","\n","\n","* __d) Short Answer on Digital Campus:__ Note that passing the optional parameter `seed` to the Spark method `randomSplit()` allows us to pseudo randomize our test/train split in a way that is replicable. Run the code similar to 'Q5c' in the line where we perform the `normedRDD.randomSplit()` with the seed set to `seed = 5` and `seed = 4`.  What is the difference you notice between the plots in Q5c, and in Q5d when the seed is set to 5 and 4? Explain why you think this difference occurs and propose at least one approach to give us more reliable results in the future."]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"997f7ee1-7eb3-42ba-84e9-0e13cdf9b119","showTitle":false,"title":""}},"source":["### Q5 Student Answers:\n","\n","OPTIONAL - if you'd like to save your answers in this notebook: \n","\n","> __a)__ Type your answer here!  \n","\n","> __c)__ Type your answer here! \n","\n","> __d)__ Type your answer here!"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"96daef89-bfa6-459e-8251-6687f607718a","showTitle":false,"title":""}},"outputs":[],"source":["# part b - OLS gradient descent function\n","def GradientDescent(trainRDD, testRDD, wInit, nSteps = 20, \n","                    learningRate = 0.1, verbose = False):\n","    \"\"\"\n","    Perform nSteps iterations of OLS gradient descent and \n","    track loss on a test and train set. Return lists of\n","    test/train loss and the models themselves.\n","    \"\"\"\n","    # initialize lists to track model performance\n","    train_history, test_history, model_history = [], [], []\n","    \n","    # perform n updates & compute test and train loss after each\n","    model = wInit\n","    for idx in range(nSteps): \n","        \n","        ############## YOUR CODE HERE #############\n","        model = None\n","        training_loss = None\n","        test_loss = None\n","        ############## (END) YOUR CODE #############\n","        \n","        # keep track of test/train loss for plotting\n","        train_history.append(training_loss)\n","        test_history.append(test_loss)\n","        model_history.append(model)\n","        \n","        # console output if desired\n","        if verbose:\n","            print(\"----------\")\n","            print(f\"STEP: {idx+1}\")\n","            print(f\"training loss: {training_loss}\")\n","            print(f\"test loss: {test_loss}\")\n","            print(f\"Model: {[round(w,3) for w in model]}\")\n","    return train_history, test_history, model_history"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"9c1e18b5-4d75-45ef-a6ea-470d82cacd38","showTitle":false,"title":""}},"outputs":[],"source":["# part c - plot error curves - (RUN THIS CELL AS IS)\n","def plotErrorCurves(trainLoss, testLoss, title = None):\n","    \"\"\"\n","    Helper function for plotting.\n","    Args: trainLoss (list of MSE) , testLoss (list of MSE)\n","    \"\"\"\n","    fig, ax = plt.subplots(1,1,figsize = (16,8))\n","    x = list(range(len(trainLoss)))[1:]\n","    ax.plot(x, trainLoss[1:], 'k--', label='Training Loss')\n","    ax.plot(x, testLoss[1:], 'r--', label='Test Loss')\n","    ax.legend(loc='upper right', fontsize='x-large')\n","    plt.xlabel('Number of Iterations')\n","    plt.ylabel('Mean Squared Error')\n","    if title:\n","        plt.title(title)\n","    display(plt.show())"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"778968ef-8ca1-4e5a-9d85-53ea8bc29ed7","showTitle":false,"title":""}},"outputs":[],"source":["# part c - run 50 iterations (RUN THIS CELL AS IS)\n","wInit = BASELINE\n","trainRDD, testRDD = normedRDD.randomSplit([0.8,0.2], seed = 2018)\n","start = time.time()\n","MSEtrain, MSEtest, models = GradientDescent(trainRDD, testRDD, wInit, nSteps = 50)\n","print(f\"\\n... trained {len(models)} iterations in {time.time() - start} seconds\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"4bbda5f8-d621-4604-9497-b90c16790eeb","showTitle":false,"title":""}},"outputs":[],"source":["# part c - take a look (RUN THIS CELL AS IS)\n","plotErrorCurves(MSEtrain, MSEtest, title = 'Ordinary Least Squares Regression' )"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<b> Question 5C Expected Output <b>\n","\n","\n","<img src='https://github.com/UCB-w261/w261-environment/blob/master/hw-images/HW4%20Expected%20Outputs/Q5C%20Expected%20Output.png?raw=true' style='width:60%'>"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"989054d9-0152-409a-9da4-928105ced8a5","showTitle":false,"title":""}},"outputs":[],"source":["# part c - Save the models & their performance for comparison later (RUN THIS CELL AS IS)\n","np.savetxt('/data/OLSmodels.csv', np.array(models), delimiter=',')\n","np.savetxt('/data/OLSloss.csv', np.array([MSEtrain, MSEtest]), delimiter=',')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# part c - Put data on personal bucket (RUN THIS CELL AS IS)\n","data_loc = f'{HW4_FOLDER}/data/OLSmodels.csv'\n","!cat data/OLSmodels.csv | gsutil cp - {data_loc}\n","data_loc = f'{HW4_FOLDER}/data/OLSloss.csv'\n","!cat data/OLSloss.csv | gsutil cp - {data_loc}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# part d - run 50 iterations (RUN THIS CELL AS IS)\n","wInit = BASELINE\n","trainRDD, testRDD = normedRDD.randomSplit([0.8,0.2], seed = 5)\n","start = time.time()\n","MSEtrain, MSEtest, models = GradientDescent(trainRDD, testRDD, wInit, nSteps = 50)\n","print(f\"\\n... trained {len(models)} iterations in {time.time() - start} seconds\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# part d - take a look (RUN THIS CELL AS IS)\n","plotErrorCurves(MSEtrain, MSEtest, title = 'Ordinary Least Squares Regression' )"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<b> Question 5D Expected Output <b>\n","\n","\n","<img src='https://github.com/UCB-w261/w261-environment/blob/master/hw-images/HW4%20Expected%20Outputs/Q5D_1%20Expected%20Output.png?raw=true' style='width:60%'>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# part d - run 50 iterations (RUN THIS CELL AS IS)\n","wInit = BASELINE\n","trainRDD, testRDD = normedRDD.randomSplit([0.8,0.2], seed = 4)\n","start = time.time()\n","MSEtrain, MSEtest, models = GradientDescent(trainRDD, testRDD, wInit, nSteps = 50)\n","print(f\"\\n... trained {len(models)} iterations in {time.time() - start} seconds\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# part d - take a look (RUN THIS CELL AS IS)\n","plotErrorCurves(MSEtrain, MSEtest, title = 'Ordinary Least Squares Regression' )"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<b> Question 5D Expected Output <b>\n","\n","\n","<img src='https://github.com/UCB-w261/w261-environment/blob/master/hw-images/HW4%20Expected%20Outputs/Q5D_2%20Expected%20Output.png?raw=true' style='width:60%'>"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"3ee3ca19-5dcb-425f-8654-89f66a58585b","showTitle":false,"title":""}},"source":["# Question 6: Cross Validation\n","\n","In question 5 we mentioned that computing the loss after each iteration is not strictly a part of Gradient Descent, its just convenient for visualizing our progress. This \"third step\" however comes with a tradeoff: it requires an extra pass through the data. Normally this would cause us to cringe except for the fact that both the loss computation and the gradient computation are very easy to parallelize - lots of the work can be done in place where no shuffle is needed for the aggregation. \n","\n","[Cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)), sometimes called rotation estimation, or out-of-sample testing. It is a model validation technique for assessing how well the model will generalize to an independent data set. The goal of cross-validation is to test the model's ability to predict new data. \n","\n","Cross validation, which will solve the problem of the unreliable test-loss that we saw in question 5, presents a bit more of a scalability challenge. To avoid over-dependence on a particulary good or bad train/test split we divide the data into `k` roughly equal size parts and train `k` models. The `k-th` model is trained on all the data _except_ the `k-th` split which is used as a test set for that model. Finally we compute the loss by averaging together the train/test loss for each model. In this question we've provided a code base to perform gradient descent and cross validation in parallel. You'll fill in some of the key details based on your understanding from questions 1-5.\n","\n","#### From ISLR Chapter 5.1 - Cross Validation\n","<img src=\"https://github.com/kyleiwaniec/w261_assets/blob/master/images/CV-ISLRp181.png?raw=true\">\n","\n","\n","### Q6 Tasks:\n","* __a) Numerical answer on Digital Campus:__ A naive approach to training an OLS Regression model with cross validation might be to simply perform Gradient Descent on each of the 5 models in sequence. In this naive approach, how many total passes would be made over the data for training? \n","\n","    [__`HINT:`__ _it will depend on the number of factors/ operations required during training, and does not include calculating the test loss_]\n","\n","\n","* __b) Multiple Dropdowns on Digital Campus:__ Read through the provided helper function `kResiduals()` and note where it gets used in the subsequent function `CVLoss()`. \n","    * For each record in the original dataset, how many tuples does `kResiduals()` emit for each model? \n","    * What are the keys of these newly emitted records?\n","\n","\n","* __c) Short Answer on Digital Campus:__ How will these keys help us compute cross validated loss?\n","\n","\n","* __d) Code:__ Complete the missing Spark code in `CVLoss()` so that this function returns the test/train cross validated error for a given set of data splits, and their corresponding models. \n","\n","    [__`TIP:`__ _your goal is to start from `partialLossRDD` and compute the test & train loss for each model so that the provided code can take the final average_].\n","\n","\n","* __e) Code:__ Read through the provided functions `partialGradients()` and `CVUpdate()`. These should have a familiar feel. Fill in the missing line in `CVUpdate()` to update each model and add the (new) array of coefficients to the `new_models` list. \n","\n","\n","* __f) Multiple Choice on Digital Campus:__ Read `GradientDescent_withCV()` and then run the provided code to perform 50 iterations and plot the error curves. What can you conclude from this graph?"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"aa355ad6-eec8-40f1-8c76-bfe320aa7657","showTitle":false,"title":""}},"source":["### Q6 Student Answers:\n","\n","OPTIONAL - if you'd like to save your answers in this notebook: \n","\n","> __a)__ Type your answer here!  \n","\n","> __b)__ Type your answer here! \n","\n","> __c)__ Type your answer here!\n","\n","> __f)__ Type your answer here!"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"a54f785c-aae5-47bf-8346-1ae48f874f4a","showTitle":false,"title":""}},"outputs":[],"source":["# part b - helper function to emit residuals (RUN THIS CELL AS IS)\n","def kResiduals(dataPoint, models, splitNum):\n","    \"\"\"\n","    Compute the (squared) residuals for a data point given k different models.\n","    Note that points from the k-th split are part of the test set for model number k\n","    and part of the training set for all other models. We'll emit a key to track this.\n","    Args:\n","        dataPoint - tuple of (features_array, y)\n","        models    - list of arrays representing model weights (bias at index 0)\n","    Returns:\n","        (stringFormattedKey, squared_error)\n","    \"\"\"\n","    # augment the data point with a bias term at index 0\n","    X = np.append([1.0], dataPoint[0])\n","    y = dataPoint[1]\n","    # emit squared residuals for each model\n","    for modelNum, W in enumerate(models):\n","        if modelNum == splitNum:\n","            yield(f\"{modelNum}-test\", (W.dot(X) - y)**2)\n","        else:\n","            yield(f\"{modelNum}-train\", (W.dot(X) - y)**2)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"6765b7f2-a243-4d96-bc33-067ac20d6b82","showTitle":false,"title":""}},"outputs":[],"source":["# part d - fill in the missing code below\n","def CVLoss(dataSplits, models):\n","    \"\"\"\n","    Compute the k-fold cross-validated test and train loss.\n","    Args:\n","        dataSplits - list of RDDs corresponding to the k test splits.\n","        models     - list of k arrays representing model weights (bias at index 0)\n","    Returns: \n","        tuple of floats: (training_loss, test_loss)\n","    \"\"\"\n","    # compute k residuals for each dataPoint (one for each model)\n","    partialLossRDD = sc.parallelize([])\n","    for splitNum, splitRDD in enumerate(dataSplits):\n","        residuals = splitRDD.flatMap(lambda x: kResiduals(x, models, splitNum))\n","        partialLossRDD = sc.union([partialLossRDD, residuals])\n","    \n","    ################ YOUR CODE HERE #################        \n","    loss = None\n","    \n","    \n","    \n","    \n","    ################ (END) YOUR CODE ################# \n","    \n","    test_loss = np.mean([x[1] for x in loss if x[0].split('-')[1] == 'test'])\n","    training_loss = np.mean([x[1] for x in loss if x[0].split('-')[1] == 'train'])\n","    return training_loss, test_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"f517e997-3621-473f-8d04-371c66ae5624","showTitle":false,"title":""}},"outputs":[],"source":["# part e - helper function (RUN THIS CELL AS IS)\n","def partialGradients(splitNum, dataPoint, models):\n","    \"\"\"\n","    Emit partial gradient for this data point for each model.\n","    NOTE: a data point from split-number k is in the test set for \n","    model-k so we don't compute a partial gradient for that model.\n","    \"\"\"\n","    # augment the data point\n","    X = np.append([1.0], dataPoint[0])\n","    y = dataPoint[1]\n","    # emit partial gradients for each model with a counter for averaging later\n","    for modelNum, W in enumerate(models):\n","        if modelNum != splitNum:\n","            yield (modelNum, [(W.dot(X) - y)*X, 1])"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"9f8dd6ca-1bf8-44ad-8dcd-1eced5d5dae8","showTitle":false,"title":""}},"outputs":[],"source":["# part e - perform GD updates for all k models (FILL IN MISSING CODE BELOW)\n","def CVUpdate(dataSplits, models, learningRate = 0.1):\n","    \"\"\"\n","    Compute gradients for k models given k corresponding dataSplits.\n","    NOTE: the training set for model-k is all records EXCEPT those in the k-th split.\n","    \"\"\"\n","    # compute partial gradient k-1 times for each fold\n","    partialsRDD = sc.parallelize([])\n","    for splitNum, splitRDD in enumerate(dataSplits):\n","        thisFoldPartialGrads = splitRDD.flatMap(lambda x: partialGradients(splitNum, x, models))\n","        partialsRDD = sc.union([partialsRDD, thisFoldPartialGrads])\n","\n","    # compute gradients by taking the average partialGrad for each fold\n","    gradients = partialsRDD.reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\\\n","                           .mapValues(lambda x: x[0]/x[1])\\\n","                           .map(lambda x: x[1])\\\n","                           .collect()\n","    \n","    # update all k models & return them in a list\n","    new_models = []\n","    for W, grad in zip(models, gradients):\n","        ############# YOUR CODE HERE ############\n","        pass\n","        ############# (END) YOUR CODE ###########\n","    return new_models"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"edc26814-f025-42da-a0a9-be9d6f22977b","showTitle":false,"title":""}},"outputs":[],"source":["# part f - (RUN THIS CELL AS IS)\n","def GradientDescent_withCV(dataSplits, wInit, learningRate=0.1, nSteps = 5, verbose = False):\n","    \"\"\"\n","    Train k models in parallel and track cross validated test/train loss.\n","    Returns:\n","        train_hist - (list) of floats\n","        test_hist - (list) of floats\n","        model_hist - (list) of arrays representing model coefficients (bias at index 0)\n","    \"\"\"\n","    # broadcast initial models (one for each fold)\n","    bModels = sc.broadcast([wInit] * len(dataSplits))\n","    \n","    \n","    # initialize lists to track performance\n","    train_loss_0, test_loss_0 = CVLoss(dataSplits, bModels.value)\n","    train_hist, test_hist, model_hist = [train_loss_0], [test_loss_0], [wInit]\n","    \n","    # perform k gradient updates at a time (one for each fold)\n","    start = time.time()\n","    for step in range(nSteps):\n","        new_models = CVUpdate(dataSplits, bModels.value, learningRate)\n","           \n","        bModels = sc.broadcast(new_models)\n","\n","        # log progress\n","        train_loss, test_loss = CVLoss(dataSplits, bModels.value)\n","        train_hist.append(train_loss)\n","        test_hist.append(test_loss)\n","        model_hist.append(new_models[0])\n","        \n","        if verbose:\n","            print(\"-------------------\")\n","            print(f\"STEP {step}: \")\n","            print(f\"model 1: {[round(w,4) for w in new_models[0]]}\")\n","            print(f\" train loss: {round(train_loss,4)}\")\n","            print(f\" test loss: {round(test_loss,4)}\")\n","            \n","    print(f\"\\n... trained {nSteps} iterations in {time.time() - start} seconds\")\n","    return train_hist, test_hist, model_hist\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"842c3024-ed16-4e24-922b-4df751b135b6","showTitle":false,"title":""}},"outputs":[],"source":["# part f -  run 50 iterations (RUN THIS CELL AS IS)\n","dataSplits = normedRDD.randomSplit([0.2, 0.2, 0.2, 0.2, 0.2], seed = 2018) \n","wInit = BASELINE\n","trainLoss, testLoss, models = GradientDescent_withCV(dataSplits, wInit, learningRate=0.1, nSteps = 50, verbose = False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# part f - take a look (RUN THIS CELL AS IS)\n","plotErrorCurves(trainLoss, testLoss, title = '5-fold Cross Validated Loss' )"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<b> Question 6F Expected Output <b>\n","\n","\n","<img src='https://github.com/UCB-w261/w261-environment/blob/master/hw-images/HW4%20Expected%20Outputs/Q6%20Expected%20Output.png?raw=true' style='width:60%'>"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"2c9a0c0c-7bc5-4054-8e63-0d72dfbdd84a","showTitle":false,"title":""}},"source":["# Question 7: Regularization.\n","\n","Our goal, as always, is to build a linear model that will extend well to unseen data. Chosing the right combination of features to optimize generalizability can be extremely computationally costly given that there are \\\\(2^{p}\\\\) potential models that can be built from \\\\(p\\\\) features. Traditional methods like forward selection would involve iteratively testing these options to asses which combinations of features achieve a statistically significant prediction.\n","\n","Ridge Regression and Lasso Regression are two popular alternatives to OLS, which enable us to train generalizable models without the trouble of forward selection and/or manual feature selection.  Both methods take advantage of the bias-variance tradeoff by _shrinking_ the model coefficients towards 0 which reduces the variance of our model with little increase in bias. In practice this 'shrinkage' is achieved by adding a penalty (a.k.a. 'regularization') term to the means squared error loss function. In this question you will implement Gradient Descent with ridge and lasso regularization.\n","\n","__`IMPORTANT NOTE:`__ When performing regularization _do not_ include the bias in your regularization term calcultion (Recall, that throughout this assignment we've included the bias at index 0 in the vector of weights that is your model).\n","\n","### Q7 Tasks:\n","* __a) Multiple Choce on Digital Campus:__ Identify the formula for the loss function for Ridge Regularization.\n","\n","* __b) Multiple Choice on Digital Campus:__ Identify the formula for the gradient function for Ridge Regularization.\n","\n","* __c) Multiple Choice on Digital Campus:__ Identify the formula for the loss function for Lasso Regularization.\n","\n","* __d) Multiple Choice on Digital Campus:__ Identify the formula for the gradient function for Lasso Regularization.\n","\n","* __e) Code:__ Fill in the first two missing code blocks in `GDUpdate_wReg()` so that this function will perform a single parameter update using \\\\(L2\\\\) regularization if the parameter `regType` is set to `ridge`, \\\\(L1\\\\) regularization if set to `lasso` and unregularized OLS otherwise.\n","\n","* __f) Code + Short Answer on Digital Campus:__ Use the provided code to train 50 iterations of ridge and lasso regression and plot the test/train error. Comment on the curves you see. Does this match your expectation? Explain why or why not."]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"02ffd797-a25b-438f-a12e-16900ad3100f","showTitle":false,"title":""}},"source":["### Q7 Student Answers:\n","\n","OPTIONAL - if you'd like to save your answers in this notebook: \n","\n","> __a)__ Type your answer here!  \n","\n","> __b)__ Type your answer here!  \n","\n","> __c)__ Type your answer here!  \n","\n","> __d)__ Type your answer here!\n","\n","> __f)__ Type your answer here!"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# <--- OPTIONAL ---> \n","# part e - This code block is for your reference, to check the difference between using np.sign() \n","# and the (wReg>0).astype(int) * 2 - 1 output\n","import numpy as np\n","W=np.array([1,-2,0,4])\n","print(2*np.append([0.0], W[1:]))\n","print(W)\n","\n","# np.sign() method returns 0 for any record that is 0\n","wreg=np.sign(W)\n","wreg[0]=0\n","print(wreg)\n","\n","# Using .astype(int) method returns -1 for any record that is 0\n","wReg = W * 1\n","wReg=(wReg>0).astype(int) * 2-1\n","wReg[0] = 0\n","print(wReg)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"aa766d8c-00d8-4b42-879b-506462670492","showTitle":false,"title":""}},"outputs":[],"source":["# part e - gradient descent with regularization\n","def GDUpdate_wReg(dataRDD, W, learningRate = 0.1, regType = None, regParam = 0.1):\n","    \"\"\"\n","    Perform one gradient descent step/update with ridge or lasso regularization.\n","    Args:\n","        dataRDD - tuple of (features_array, y)\n","        W       - (array) model coefficients with bias at index 0\n","        learningRate - (float) defaults to 0.1\n","        regType - (str) 'ridge' or 'lasso', defaults to None\n","        regParam - (float) regularization term coefficient\n","    Returns:\n","        model   - (array) updated coefficients, bias still at index 0\n","    \"\"\"\n","    # augmented data\n","    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[0]), x[1]))\n","    \n","    new_model = None\n","    #################### YOUR CODE HERE ###################\n","\n","    \n","    \n","    \n","    \n","    ################## (END) YOUR CODE ####################\n","    return new_model"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"a37b7b07-8b70-4eba-8c9e-61c78f7444cb","showTitle":false,"title":""}},"outputs":[],"source":["# part e - ridge/lasso gradient descent function (RUN THIS CELL AS IS)\n","def GradientDescent_wReg(trainRDD, testRDD, wInit, nSteps = 20, learningRate = 0.1,\n","                         regType = None, regParam = 0.1, verbose = False):\n","    \"\"\"\n","    Perform nSteps iterations of regularized gradient descent and \n","    track loss on a test and train set. Return lists of\n","    test/train loss and the models themselves.\n","    \"\"\"\n","    # initialize lists to track model performance\n","    train_history, test_history, model_history = [], [], []\n","    \n","    # perform n updates & compute test and train loss after each\n","    model = wInit\n","    for idx in range(nSteps):  \n","        # update the model\n","        model = GDUpdate_wReg(trainRDD, model, learningRate, regType, regParam)\n","        \n","        # keep track of test/train loss for plotting\n","        train_history.append(OLSLoss(trainRDD, model))\n","        test_history.append(OLSLoss(testRDD, model))\n","        model_history.append(model)\n","        \n","        # console output if desired\n","        if verbose:\n","            print(\"----------\")\n","            print(f\"STEP: {idx+1}\")\n","            print(f\"training loss: {training_loss}\")\n","            print(f\"test loss: {test_loss}\")\n","            print(f\"Model: {[round(w,3) for w in model]}\")\n","    return train_history, test_history, model_history"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"8e5748b0-545b-4f48-abde-2b2fe3821a53","showTitle":false,"title":""}},"outputs":[],"source":["# part f - run 50 iterations of ridge (RUN THIS CELL AS IS)\n","wInit = BASELINE\n","trainRDD, testRDD = normedRDD.randomSplit([0.8,0.2], seed = 5)\n","start = time.time()\n","ridge_results = GradientDescent_wReg(trainRDD, testRDD, wInit, nSteps = 50, \n","                                     regType='ridge', regParam = 0.05 )\n","print(f\"\\n... trained {len(ridge_results[2])} iterations in {time.time() - start} seconds\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"faa89abc-d75c-4093-9fcf-c89f14072710","showTitle":false,"title":""}},"outputs":[],"source":["# part f - save and display ridge results (RUN THIS CELL AS IS)\n","trainLoss, testLoss, models = ridge_results\n","np.savetxt('/data/ridge_models.csv', np.array(models), delimiter=',')\n","np.savetxt('/data/ridge_loss.csv', np.array([trainLoss, testLoss]), delimiter=',')\n","plotErrorCurves(trainLoss, testLoss, title = 'Ridge Regression Error Curves' )"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<b> Question 7F Expected Output <b>\n","\n","\n","<img src='https://github.com/UCB-w261/w261-environment/blob/master/hw-images/HW4%20Expected%20Outputs/Q7%20Ridge%20Expected%20Output.png?raw=true' style='width:60%'>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# part f - Put data on personal bucket (RUN THIS CELL AS IS)\n","data_loc = f'{HW4_FOLDER}/data/ridge_models.csv'\n","!cat data/ridge_models.csv | gsutil cp - {data_loc}\n","data_loc = f'{HW4_FOLDER}/data/ridge_loss.csv'\n","!cat data/ridge_loss.csv | gsutil cp - {data_loc}"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"f8ec4c04-46b6-4ec8-87b6-208a65eb0234","showTitle":false,"title":""}},"outputs":[],"source":["# part f - run 50 iterations of lasso (RUN THIS CELL AS IS)\n","wInit = BASELINE\n","trainRDD, testRDD = normedRDD.randomSplit([0.8,0.2], seed = 5)\n","start = time.time()\n","lasso_results = GradientDescent_wReg(trainRDD, testRDD, wInit, nSteps = 50,\n","                                     regType='lasso', regParam = 0.05)\n","print(f\"\\n... trained {len(lasso_results[2])} iterations in {time.time() - start} seconds\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"83ae8d15-4c71-4738-b7c5-6d8b51291b7c","showTitle":false,"title":""}},"outputs":[],"source":["# part f - save and display lasso results (RUN THIS CELL AS IS)\n","trainLoss, testLoss, models = lasso_results\n","np.savetxt('/data/lasso_models.csv', np.array(models), delimiter=',')\n","np.savetxt('/data/lasso_loss.csv', np.array([trainLoss, testLoss]), delimiter=',')\n","plotErrorCurves(trainLoss, testLoss, title = 'Lasso Regression Error Curves' )"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<b> Question 7F Expected Output <b>\n","\n","\n","<img src='https://github.com/UCB-w261/w261-environment/blob/master/hw-images/HW4%20Expected%20Outputs/Q7%20Lasso%20Expected%20Output.png?raw=true' style='width:60%'>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# part f - Put data on personal bucket (RUN THIS CELL AS IS)\n","data_loc = f'{HW4_FOLDER}/data/lasso_models.csv'\n","!cat data/lasso_models.csv | gsutil cp - {data_loc}\n","data_loc = f'{HW4_FOLDER}/data/lasso_loss.csv'\n","!cat data/lasso_loss.csv | gsutil cp - {data_loc}"]},{"cell_type":"markdown","metadata":{},"source":["# Question 8: Results\n","\n","In this question we'll use a few different plots to help us compare the OLS, Ridge and Lasso models that we have trained. Use the provided code to load the training history from file and retrieve the best (i.e. last) model from each method.\n","\n","### Q8 Tasks:\n","* __a) Code:__ Use the provided code to load the training history from files and retrieve the best (i.e. last) model from each method. Then create a new RDD called `validationRDD` from the held out data, and compute the mean squared error (MSE) on the new validation dataset for each of the three models (OLS, Ridge and Lasso). You don't need to include the regularization loss in the MSE calculations.\n","\n","    [__`TIP:`__ _the held out data is in its raw form, don't forget to parse and normalize before applying your calculations, you should also be careful to normalize using the same scaling parameters that you used for the training data._]\n","\n","* __b) Numeric Answer on Digital Campus:__ Enter the Mean Squared Error for the OLS model (to at least 4 decimal places).\n","\n","* __c) Numeric Answer on Digital Campus:__ Enter the Mean Squared Error for the Ridge model (to at least 4 decimal places).\n","\n","* __d) Numeric Answer on Digital Campus:__ Enter the Mean Squared Error for the Lasso model (to at least 4 decimal places).\n","\n","* __e) Multiple Choice on Digital Campus:__ Which model performed best and what is a valid possible interpretation of these results?\n","\n","* __f) Multiple Choice on Digital Campus:__ Use the provided code to plot side by side boxplots of the residuals vs. the outcome (i.e. `quality`). What can you observe about our model performance? \n","    [__`TIP:`__ _note that the heldout data set is plenty small enough to fit in memory so no need to sample. Feel free to do your plotting in pandas or any other comfortable python package._]\n","\n","* __g) Multiple Choice on Digital Campus:__ Run the provided code to visualize the model coefficients for the first 50 iterations of training. What do you observe about how the OLS, ridge and lasso coefficients change over the course of the training process?"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"398e36af-a2cc-4869-81e2-39b41acf25cb","showTitle":false,"title":""}},"source":["### Q8 Student Answers:\n","\n","OPTIONAL - if you'd like to save your answers in this notebook: \n","\n","> __a)__ Type your answer here!  \n","\n","> __b)__ Type your answer here!  \n","\n","> __c)__ Type your answer here!\n","\n","> __d)__ Type your answer here!  \n","\n","> __e)__ Type your answer here!  \n","\n","> __f)__ Type your answer here!\n","\n","> __g)__ Type your answer here!"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"a43791fb-9f8f-4d27-a86f-e1ad451178c7","showTitle":false,"title":""}},"outputs":[],"source":["# part a - load the models from file (RUN THIS CELL AS IS)\n","data_loc = !gsutil cat {HW4_FOLDER}/data/OLSmodels.csv\n","ols_models = np.loadtxt(data_loc, dtype=float, delimiter=',')\n","\n","data_loc = !gsutil cat {HW4_FOLDER}/data/ridge_models.csv\n","ridge_models = np.loadtxt(data_loc, dtype=float, delimiter=',')\n","\n","data_loc = !gsutil cat {HW4_FOLDER}/data/lasso_models.csv\n","lasso_models = np.loadtxt(data_loc, dtype=float, delimiter=',')\n","\n","best_ols = ols_models[-1,:]\n","best_ridge = ridge_models[-1,:]\n","best_lasso = lasso_models[-1,:]"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"2a3a81d4-a3ff-4ec5-8c1b-525a2f9cc147","showTitle":false,"title":""}},"outputs":[],"source":["# part a - compute MSE on the held out data for all three 'best' models\n","olsMSE, ridgeMSE, lassoMSE = None, None, None\n","validationRDD = None\n","############### YOUR CODE HERE #################\n","\n","\n","\n","\n","\n","\n","############### YOUR CODE HERE #################\n","\n","print(f\"OLS Mean Squared Error: {olsMSE}\")\n","print(f\"Ridge Mean Squared Error: {ridgeMSE}\")\n","print(f\"Lasso Mean Squared Error: {lassoMSE}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"f332b280-3fe7-4352-8a08-36ca69f078f3","showTitle":false,"title":""}},"outputs":[],"source":["# part f - helper function (RUN THIS CELL AS IS)\n","def get_residuals(dataRDD, model):\n","    \"\"\"\n","    Return a collected list of tuples (residual, quality_score)\n","    \"\"\"\n","    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[0]), x[1]))\n","    residuals = augmentedData.map(lambda x: (x[1] - model.dot(x[0]), x[1]))\n","    return residuals.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"89363889-24de-45ce-b471-db773935ef9a","showTitle":false,"title":""}},"outputs":[],"source":["# part f - compute residuals for all three models (RUN THIS CELL AS IS)\n","ols_resid = np.array(get_residuals(validationRDD, best_ols))\n","ridge_resid = np.array(get_residuals(validationRDD, best_ridge))\n","lasso_resid = np.array(get_residuals(validationRDD, best_lasso))"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"09eb6bdc-e7a9-48ec-9893-58e7072e144d","showTitle":false,"title":""}},"outputs":[],"source":["# part f - boxplots of residuals for all three models (RUN THIS CELL AS IS)\n","fig, axes = plt.subplots(1, 3, figsize=(15,5))\n","stuff_to_plot = zip(axes, [\"OLS\", \"Ridge\", \"Lasso\"], [ols_resid, ridge_resid, lasso_resid])\n","for ax, title, data in stuff_to_plot:\n","    ax.set_title(title)\n","    y = data[:, 0]\n","    x = data[:, 1]\n","    sns.boxplot(x=x, y=y, ax=ax)\n","fig.suptitle(\"Prediction Error vs. Quality Score\", fontsize=15, y=0.98)\n","display(plt.show())"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"2dd62903-3a50-4593-b7f4-7a9f598f9f88","showTitle":false,"title":""}},"outputs":[],"source":["# part g - plotting function (RUN THIS CELL AS IS)\n","def plotCoeffs(models, featureNames, title):\n","    \"\"\"\n","    Helper Function to show how coefficients change as we train.\n","    \"\"\"\n","    fig, ax = plt.subplots(figsize = (15,8))\n","    X = list(range(len(models)))\n","    for data, name in zip(models.T, featureNames):\n","        if name == \"Bias\":\n","            continue\n","        ax.plot(X, data, label=name)\n","    ax.plot(X,[0]*len(X), 'k--')\n","    plt.title(title)\n","    plt.legend()\n","    display(plt.show())"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"96fb842d-4f12-4ade-8a7b-7e34ef9a0889","showTitle":false,"title":""}},"outputs":[],"source":["# take a look (RUN THIS CELL AS IS)\n","plotCoeffs(ols_models, ['Bias'] + FIELDS, \"OLS Coefficients over 50 GD steps\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"4b31d9b6-fee9-447f-b047-985dbc812b60","showTitle":false,"title":""}},"outputs":[],"source":["# take a look (RUN THIS CELL AS IS)\n","plotCoeffs(ridge_models, ['Bias'] + FIELDS, \"Ridge Coefficients over 50 GD steps\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"10c132d3-5c13-4218-b3b5-2d7b0d3d7b6f","showTitle":false,"title":""}},"outputs":[],"source":["# take a look (RUN THIS CELL AS IS)\n","plotCoeffs(lasso_models, ['Bias'] + FIELDS, \"Lasso Coefficients over 50 GD steps\")"]},{"cell_type":"markdown","metadata":{},"source":["# Question 9: DataFrames and MLlib\n","\n","In this final question we'll confirm our results using DataFrames and the MLlib library, that you will use for your final Project. You can find the documentation [here](https://spark.apache.org/docs/3.1.3/api/python/reference/index.html).\n","\n","### Q9 Tasks:\n","* __a1) Code:__ Transform the un-normalized RDD `trainRDDCached` from the section *Load data and do EDA* into a Spark Dataframe. Then display the top 10 rows of the Spark Dataframe as a Pandas Dataframe.\n","* __a2) Numeric Answer on Digital Campus:__ What is the pH of the first wine (the wine at index 0) in the dataframe you've just created (to at least 2 decimal places)?\n","\n","\n","* __b1) Code:__ Normalize the Spark Dataframe and show us the top 10 rows. You *don't* need to display the dataframe as a pandas dataframe here.\n","* __b2) Numeric Answer On Digital Campus:__ What is the first normalized (scaled) feature value for the first wine in the dataframe (to at least 4 decimal places)?\n","\n","\n","* __c1) Code:__ Build a Linear Regression Model using `pyspark.ml.regression` and train for 50 iterations. (Hint: set maxIter=50])\n","* __c2) Numeric Answer On Digital Campus:__ What is the MSE for the Linear Regression Model you just wrote (to at least 4 decimal places)?\n","\n","\n","* __d1) code:__ Build a Lasso and Ridge Model using `pyspark.ml.regression` and train for 50 iterations.\n","    \n","    [`Hint`: _use following parameters when you call LinearRegression_]\n","    \n","    ```\n","    ridge = LinearRegression(featuresCol = '<name_of_featuresCol>', labelCol='<name_of_labelCol>', maxIter=50, regParam = 0.1, elasticNetParam = reg_par[0])\n","\n","    lasso = LinearRegression(featuresCol = '<name_of_featuresCol>', labelCol='<name_of_labelCol>', maxIter=50, regParam = 0.1, elasticNetParam = reg_par[1])\n","    ```\n","    <br>\n","* __d2) Numeric Answer On Digital Campus:__ What is the MSE for the Linear Regression Model with __Lasso__ regularization you just wrote (to at least 4 decimal places)?\n","\n","\n","* __e1) Code:__ Evaluate your models with the held-out test set for the basic Linear Regression, Lasso and Ridge models.\n","* __e2) Multiple Choice On Digital Campus:__ Given the choices of basic linear regression, linear regression with Lasso regularization, and linear regression with Ridge regularization, which model did the best?"]},{"cell_type":"markdown","metadata":{},"source":["### Q9 Student Answers:\n","\n","OPTIONAL - if you'd like to save your answers in this notebook: \n","\n","> __a2)__ Type your answer here!  \n","\n","> __b2)__ Type your answer here!  \n","\n","> __c2)__ Type your answer here!\n","\n","> __d2)__ Type your answer here!  \n","\n","> __e2)__ Type your answer here!  \n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Part a - From RDD to DataFrame\n","columns = [\"wine type\", \"fixed acidity\", \"volatile acidity\", \"citric acid\", \"residual sugar\", \"chlorides\", \"free sulfur dioxide\", \"total sulfur dioxide\", \"density\", \"pH\", \"sulphates\", \"alcohol\", \"quality\"]\n","############### YOUR CODE HERE #################\n","trainDF = None\n","\n","\n","\n","\n","############### YOUR CODE HERE #################\n","# trainDF.limit(10).toPandas()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Part b - Normalize DF\n","# We first need to create a Vector Assembler to create the feature space that will be fed to ML models\n","from pyspark.ml.feature import VectorAssembler\n","from pyspark.ml.feature import StandardScaler\n","\n","features = [\"fixed acidity\", \"volatile acidity\", \"citric acid\", \"residual sugar\", \"chlorides\", \"free sulfur dioxide\", \"total sulfur dioxide\", \"density\", \"pH\", \"sulphates\", \"alcohol\"]\n","assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n","trainDF_VA = assembler.transform(trainDF)\n","\n","############### YOUR CODE HERE #################\n","scaler = None\n","trainDF_VA = None\n","\n","############### YOUR CODE HERE #################\n","# trainDF_VA.limit(10).toPandas()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Part c - Train Linear Regression Model\n","from pyspark.ml.regression import LinearRegression\n","trainDF_VA = trainDF_VA.select(['features_scaled', 'quality'])\n","\n","############### YOUR CODE HERE #################\n","lr = None\n","lr_model = None\n","trainingSummary = None\n","############### YOUR CODE HERE #################"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Part c - (RUN THIS CELL AS IS)\n","print(\"Coefficients: \" + str(np.round(lr_model.coefficients, 4)))\n","print(\"Intercept: \" + str(round(lr_model.intercept, 4)))\n","print(\"MSE: %f\" % trainingSummary.rootMeanSquaredError**2)\n","print(\"r2: %f\" % trainingSummary.r2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Part d - LASSO and Ridge\n","reg_par = [0, 1] # 0 Corresponds to Ridge, 1 correspond to LASSO\n","############### YOUR CODE HERE #################\n","## Ridge\n","ridge = None\n","ridge_model = None\n","RidgeSummary = None\n","\n","## LASSO\n","lasso = None\n","lasso_model = None\n","LassoSummary = None\n","############### YOUR CODE HERE #################"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Part d - (RUN THIS CELL AS IS)\n","print(\"Ridge Model:\")\n","print(\"\\t Coefficients: \" + str(np.round(ridge_model.coefficients, 4)))\n","print(\"\\t Intercept: \" + str(round(ridge_model.intercept, 4)))\n","print(\"\\t MSE: %f\" % RidgeSummary.rootMeanSquaredError**2)\n","print(\"\\t r2: %f\" % RidgeSummary.r2)\n","print(\"\\n\")\n","print(\"LASSO Model:\")\n","print(\"\\t Coefficients: \" + str(np.round(lasso_model.coefficients, 4)))\n","print(\"\\t Intercept: \" + str(round(lasso_model.intercept, 4)))\n","print(\"\\t MSE: %f\" % LassoSummary.rootMeanSquaredError**2)\n","print(\"\\t r2: %f\" % LassoSummary.r2)\n","print(\"\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Part e - Evaluate Held Out Set\n","\n","# Create Held Out DataFrame\n","############### YOUR CODE HERE #################\n","heldOutDF = None\n","\n","############### YOUR CODE HERE #################\n","\n","\n","# Apply VectorAssembler and Normalize Held Out\n","############### YOUR CODE HERE #################\n","heldOutDF_VA = None\n","\n","############### YOUR CODE HERE #################\n","\n","\n","# Predict and Evaluate Performance\n","############### YOUR CODE HERE #################\n","# Predictions\n","lr_predictions = None\n","ridge_predictions = None\n","lasso_predictions = None\n","\n","# MSE\n","lr_mse = None\n","ridge_mse = None\n","lasso_mse = None\n","\n","############### YOUR CODE HERE #################"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Part e - (RUN THIS CELL AS IS)\n","print(\"Linear Regression Model:\")\n","print(f\"\\t MSE: {lr_mse}\")\n","print(\"\\n\")\n","print(\"Ridge Model:\")\n","print(f\"\\t MSE: {ridge_mse}\")\n","print(\"\\n\")\n","print(\"LASSO Model:\")\n","print(f\"\\t MSE: {lasso_mse}\")\n","print(\"\\n\")"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"a8abfadd-3842-4961-abe0-38d1a3af3b41","showTitle":false,"title":""}},"source":["### Congratulations, you have completed HW4!\n","\n","If you would like to provide feedback regarding this homework, please use the survey at: https://docs.google.com/forms/d/e/1FAIpQLSce9feiQeSkdP43A0ZYui1tMGIBfLfzb0rmgToQeZD9bXXX8Q/viewform"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookName":"hw4_Workbook","notebookOrigID":3159780226953918,"widgets":{}},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.15"},"vscode":{"interpreter":{"hash":"156252221e2123d4604f65e286a50c01040f0cf338cde0bac78be89dff1431fd"}}},"nbformat":4,"nbformat_minor":4}
