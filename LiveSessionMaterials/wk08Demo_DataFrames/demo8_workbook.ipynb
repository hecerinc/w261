{"cells": [{"cell_type": "markdown", "id": "1652f8f1-a6be-476d-b7ca-e20378047bbd", "metadata": {}, "source": "# wk8 Demo - Advanced Spark - DataFrames and Spark SQL\n__`MIDS w261: Machine Learning at Scale | UC Berkeley School of Information | Fall 2022`__\n\nSo far we've been using Spark's low level APIs. In particular, we've been using the RDD (Resilient Distiributed Datasets) API to implement Machine Learning algorithms from scratch. This week we're going to take a look at how Spark is used in a production setting. We'll look at DataFrames, SQL, and UDFs (User Defined Functions).  As discussed previously, we still need to understand the internals of Spark and MapReduce in general to write efficient and scalable code.\n\nIn class today we'll get some practice working with larger data sets in Spark. We'll start with an introduction to efficiently storing data and approach a large dataset for analysis. After that we'll discuss a ranking problem which was covered in Chapter 6 of the High Performance Spark book and how we can apply that to our problem. We'll follow up with a discussion on things that could be done to make this more effiicent.\n* ... __describe__ differences between data serialization formats.\n* ... __choose__ a data serialization format based on use case.\n* ... __describe__ DataFrames API, GroupBy and _Spark SQL_.\n* ... __describe__ and __create__ a data pipeline for analysis.\n* ... __use__ a user defined function (UDF).\n* ... __understand__ feature engineering and aggregations in Spark.\n\n__`Additional Resources:`__ Writing performant code in Spark requires a lot of thought. Holden's High Performance Spark book covers this topic very well. In addition, Spark - The Definitive Guide, by Bill Chambers and Matei Zaharia, provides some recent developments."}, {"cell_type": "code", "execution_count": 2, "id": "1a36cc4c-b1ee-4d46-86e7-3e4bf3433cb1", "metadata": {}, "outputs": [], "source": "## Imports\nimport re\nimport json\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom pyspark.sql import SparkSession\n\napp_name = \"week8_demo\"\nmaster = \"local[*]\"\nspark = SparkSession\\\n        .builder\\\n        .appName(app_name)\\\n        .master(master)\\\n        .config(\"spark.ui.port\",\"42229\")\\\n        .getOrCreate()\nsc = spark.sparkContext\n\n## Change the working directory\n!cd /media/notebooks/student-workspace/LiveSessionMaterials/wk08Demo_DataFrames"}, {"cell_type": "code", "execution_count": 3, "id": "056032ce-3f31-4e96-b55d-ff88964f0256", "metadata": {}, "outputs": [], "source": "## Load the data\ndata = spark.read.format('bigquery') \\\n  .option('table', 'bigquery-public-data:samples.gsod') \\\n  .load()"}, {"cell_type": "markdown", "id": "b3f7b812-a524-4a0d-bd1c-ecce0d347129", "metadata": {}, "source": "## DataFrames API\n\nLet's showcase some of the important methods that we have available when working with DataFrames"}, {"cell_type": "code", "execution_count": 3, "id": "d1a80ba8-3f79-42e7-a845-a19bec033967", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "22/06/20 15:32:50 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n[Stage 0:>                                                          (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------+-----------+----+-----+---+-------------------+---------------------+------------------+--------------------------+----------------------+----------------------------------+---------------------+---------------------------------+------------------+---------------------------+------------------+---------------------------+------------------------+-------------------+------------------+------------------------+---------------+------------------------+-------------------+----------+-----+-----+-----+-----+-------+-------+\n|station_number|wban_number|year|month|day|          mean_temp|num_mean_temp_samples|    mean_dew_point|num_mean_dew_point_samples|mean_sealevel_pressure|num_mean_sealevel_pressure_samples|mean_station_pressure|num_mean_station_pressure_samples|   mean_visibility|num_mean_visibility_samples|   mean_wind_speed|num_mean_wind_speed_samples|max_sustained_wind_speed|max_gust_wind_speed|   max_temperature|max_temperature_explicit|min_temperature|min_temperature_explicit|total_precipitation|snow_depth|  fog| rain| snow| hail|thunder|tornado|\n+--------------+-----------+----+-----+---+-------------------+---------------------+------------------+--------------------------+----------------------+----------------------------------+---------------------+---------------------------------+------------------+---------------------------+------------------+---------------------------+------------------------+-------------------+------------------+------------------------+---------------+------------------------+-------------------+----------+-----+-----+-----+-----+-------+-------+\n|         39730|      99999|1929|   10| 20|  52.79999923706055|                    4|              45.5|                         4|                  null|                              null|                 null|                             null| 6.199999809265137|                          4|21.200000762939453|                          4|      29.899999618530273|               null|              50.0|                   false|           null|                    null|                0.0|      null|false|false|false|false|  false|  false|\n|         33110|      99999|1929|   12| 18|               47.5|                    4|              44.0|                         4|                  null|                              null|                 null|                             null|               2.5|                          4|              11.0|                          4|                    13.0|               null|              45.0|                   false|           null|                    null|               null|      null|false|false|false|false|  false|  false|\n|         37770|      99999|1931|    4| 24|  50.20000076293945|                    4| 44.29999923706055|                         4|                  null|                              null|                 null|                             null| 5.900000095367432|                          4|12.300000190734863|                          4|      18.100000381469727|               null|              45.0|                   false|           null|                    null|               null|      null|false|false|false|false|  false|  false|\n|        726810|      24131|1931|    6| 23|   65.0999984741211|                   24|              41.5|                         8|                  null|                              null|                 null|                             null| 48.29999923706055|                         24| 7.199999809265137|                         24|      11.100000381469727|               null|53.400001525878906|                    true|           null|                    null|                0.0|      null|false|false|false|false|  false|  false|\n|        726810|      24131|1931|    3|  2|  42.79999923706055|                   24|              31.5|                         8|                  null|                              null|                 null|                             null|  72.9000015258789|                         24| 2.299999952316284|                         24|       4.099999904632568|               null|32.400001525878906|                    true|           null|                    null|                0.0|      null|false|false|false|false|  false|  false|\n|        726810|      24131|1931|    9| 17|               67.0|                   24|              40.5|                         8|                  null|                              null|                 null|                             null| 33.79999923706055|                         24|2.4000000953674316|                         24|                     6.0|               null| 51.29999923706055|                    true|           null|                    null|                0.0|      null|false|false|false|false|  false|  false|\n|        726810|      24131|1931|    8|  7|   68.4000015258789|                   24| 37.20000076293945|                         8|                  null|                              null|                 null|                             null|27.899999618530273|                         24|               3.5|                         24|                     7.0|               null| 52.29999923706055|                    true|           null|                    null|                0.0|      null|false|false|false|false|  false|  false|\n|        726810|      24131|1932|    7| 14|   64.0999984741211|                   24|54.099998474121094|                         8|                  null|                              null|                 null|                             null|              41.0|                         24| 4.199999809265137|                         24|       8.899999618530273|               null|55.400001525878906|                    true|           null|                    null|               null|      null|false|false|false|false|  false|  false|\n|        726810|      24131|1932|   10| 23| 41.099998474121094|                   24|              31.0|                         8|                  null|                              null|                 null|                             null|              41.0|                         24| 4.300000190734863|                         24|      15.899999618530273|               null|35.400001525878906|                    true|           null|                    null|               null|      null|false|false|false|false|  false|  false|\n|        726810|      24131|1932|    1|  5| 24.600000381469727|                   24|21.100000381469727|                         8|                  null|                              null|                 null|                             null|14.800000190734863|                         24| 3.200000047683716|                         24|       4.099999904632568|               null|21.399999618530273|                    true|           null|                    null|               null|      null| true| true| true| true|   true|   true|\n|        726815|      24106|1932|    8| 27|               71.0|                   24|              null|                      null|                  null|                              null|                 null|                             null|28.600000381469727|                         24|               8.0|                         24|                    15.0|               null|62.400001525878906|                    true|           null|                    null|                0.0|      null|false|false|false|false|  false|  false|\n|        726810|      24131|1932|    8| 20|               71.0|                   24| 41.70000076293945|                         8|                  null|                              null|                 null|                             null|              32.5|                         24| 4.099999904632568|                         24|       9.899999618530273|               null|53.400001525878906|                    true|           null|                    null|                0.0|      null|false|false|false|false|  false|  false|\n|        726810|      24131|1932|    5| 21|  55.20000076293945|                   24|46.599998474121094|                         8|                  null|                              null|                 null|                             null|22.600000381469727|                         24| 5.300000190734863|                         24|                     8.0|               null|46.400001525878906|                    true|           null|                    null|               null|      null|false|false|false|false|  false|  false|\n|        370310|      99999|1933|   10| 17|  55.29999923706055|                    4|              null|                      null|                  null|                              null|                 null|                             null|               3.0|                          4|16.799999237060547|                          4|      18.100000381469727|               null|              45.0|                   false|           null|                    null|                0.0|      null|false|false|false|false|  false|  false|\n|        292310|      99999|1933|   12|  1|              -11.0|                    4|              null|                      null|                  null|                              null|                 null|                             null|               4.0|                          4|               5.0|                          4|       8.899999618530273|               null|             -27.0|                    true|           null|                    null|               null|      null|false|false|false|false|  false|  false|\n|        370310|      99999|1933|    6| 17|  62.70000076293945|                    4|              null|                      null|                  null|                              null|                 null|                             null|              null|                       null|               1.0|                          4|       1.899999976158142|               null|              55.0|                    true|           null|                    null|                0.0|      null|false|false|false|false|  false|  false|\n|        239330|      99999|1933|    6|  4|               70.5|                    4|              null|                      null|                  null|                              null|                 null|                             null| 1.899999976158142|                          4|               5.0|                          4|       8.899999618530273|               null|              59.0|                   false|           null|                    null|                0.0|      null| true| true| true| true|   true|   true|\n|        282750|      99999|1933|    1|  7|               -5.0|                    4|              null|                      null|                  null|                              null|                 null|                             null|3.4000000953674316|                          4| 3.200000047683716|                          4|       8.899999618530273|               null|             -22.0|                    true|           null|                    null|                0.0|      null|false|false|false|false|  false|  false|\n|        292310|      99999|1933|    3| 17|-10.300000190734863|                    4|              null|                      null|                  null|                              null|                 null|                             null| 1.100000023841858|                          4|              13.5|                          4|      18.100000381469727|               null|             -36.0|                    true|           null|                    null|                0.0|      null|false|false|false|false|  false|  false|\n|        370310|      99999|1933|    4| 23|               65.0|                    4|              null|                      null|                  null|                              null|                 null|                             null|              null|                       null|               4.5|                          4|       8.899999618530273|               null|              52.0|                   false|           null|                    null|                0.0|      null|false|false|false|false|  false|  false|\n+--------------+-----------+----+-----+---+-------------------+---------------------+------------------+--------------------------+----------------------+----------------------------------+---------------------+---------------------------------+------------------+---------------------------+------------------+---------------------------+------------------------+-------------------+------------------+------------------------+---------------+------------------------+-------------------+----------+-----+-----+-----+-----+-------+-------+\nonly showing top 20 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "## show\ndata.show()"}, {"cell_type": "markdown", "id": "f6ba02cf-3c2e-477a-803c-33dff279140a", "metadata": {}, "source": "Here we see `.show()`, a method that works similarly to Pandas `.head()`. You can observe that the DataFrame is stored in text, that way it's easier to distribute throughout the different executors. If you want to better display the results, we can transform the output using `.limit(n)` to a Pandas Dataframe"}, {"cell_type": "code", "execution_count": 4, "id": "36186eee-9d93-4d20-a2cb-d52d4a4f5fc7", "metadata": {}, "outputs": [{"data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>station_number</th>\n      <th>wban_number</th>\n      <th>year</th>\n      <th>month</th>\n      <th>day</th>\n      <th>mean_temp</th>\n      <th>num_mean_temp_samples</th>\n      <th>mean_dew_point</th>\n      <th>num_mean_dew_point_samples</th>\n      <th>mean_sealevel_pressure</th>\n      <th>...</th>\n      <th>min_temperature</th>\n      <th>min_temperature_explicit</th>\n      <th>total_precipitation</th>\n      <th>snow_depth</th>\n      <th>fog</th>\n      <th>rain</th>\n      <th>snow</th>\n      <th>hail</th>\n      <th>thunder</th>\n      <th>tornado</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>39730</td>\n      <td>99999</td>\n      <td>1929</td>\n      <td>10</td>\n      <td>20</td>\n      <td>52.799999</td>\n      <td>4</td>\n      <td>45.500000</td>\n      <td>4</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>None</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>33110</td>\n      <td>99999</td>\n      <td>1929</td>\n      <td>12</td>\n      <td>18</td>\n      <td>47.500000</td>\n      <td>4</td>\n      <td>44.000000</td>\n      <td>4</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>None</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>37770</td>\n      <td>99999</td>\n      <td>1931</td>\n      <td>4</td>\n      <td>24</td>\n      <td>50.200001</td>\n      <td>4</td>\n      <td>44.299999</td>\n      <td>4</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>None</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>726810</td>\n      <td>24131</td>\n      <td>1931</td>\n      <td>6</td>\n      <td>23</td>\n      <td>65.099998</td>\n      <td>24</td>\n      <td>41.500000</td>\n      <td>8</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>None</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>726810</td>\n      <td>24131</td>\n      <td>1931</td>\n      <td>3</td>\n      <td>2</td>\n      <td>42.799999</td>\n      <td>24</td>\n      <td>31.500000</td>\n      <td>8</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>None</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows \u00d7 31 columns</p>\n</div>", "text/plain": "   station_number  wban_number  year  month  day  mean_temp  \\\n0           39730        99999  1929     10   20  52.799999   \n1           33110        99999  1929     12   18  47.500000   \n2           37770        99999  1931      4   24  50.200001   \n3          726810        24131  1931      6   23  65.099998   \n4          726810        24131  1931      3    2  42.799999   \n\n   num_mean_temp_samples  mean_dew_point  num_mean_dew_point_samples  \\\n0                      4       45.500000                           4   \n1                      4       44.000000                           4   \n2                      4       44.299999                           4   \n3                     24       41.500000                           8   \n4                     24       31.500000                           8   \n\n   mean_sealevel_pressure  ...  min_temperature  min_temperature_explicit  \\\n0                     NaN  ...              NaN                      None   \n1                     NaN  ...              NaN                      None   \n2                     NaN  ...              NaN                      None   \n3                     NaN  ...              NaN                      None   \n4                     NaN  ...              NaN                      None   \n\n   total_precipitation  snow_depth    fog   rain   snow   hail  thunder  \\\n0                  0.0         NaN  False  False  False  False    False   \n1                  NaN         NaN  False  False  False  False    False   \n2                  NaN         NaN  False  False  False  False    False   \n3                  0.0         NaN  False  False  False  False    False   \n4                  0.0         NaN  False  False  False  False    False   \n\n   tornado  \n0    False  \n1    False  \n2    False  \n3    False  \n4    False  \n\n[5 rows x 31 columns]"}, "execution_count": 4, "metadata": {}, "output_type": "execute_result"}], "source": "data.limit(10).toPandas().head()"}, {"cell_type": "markdown", "id": "8b18bbac-6e60-4a4b-8b63-c4e83c473844", "metadata": {}, "source": "This is a public dataset from NOAA, regarding weather stations across the United States. It has a total of 31 columns.\n\nAnother important command is `.printSchema()` to check columns names and what type of data is stored on it"}, {"cell_type": "code", "execution_count": 5, "id": "e7b26525-6186-40ea-98c9-fb3a680c0cb9", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- station_number: long (nullable = false)\n |-- wban_number: long (nullable = true)\n |-- year: long (nullable = false)\n |-- month: long (nullable = false)\n |-- day: long (nullable = false)\n |-- mean_temp: double (nullable = true)\n |-- num_mean_temp_samples: long (nullable = true)\n |-- mean_dew_point: double (nullable = true)\n |-- num_mean_dew_point_samples: long (nullable = true)\n |-- mean_sealevel_pressure: double (nullable = true)\n |-- num_mean_sealevel_pressure_samples: long (nullable = true)\n |-- mean_station_pressure: double (nullable = true)\n |-- num_mean_station_pressure_samples: long (nullable = true)\n |-- mean_visibility: double (nullable = true)\n |-- num_mean_visibility_samples: long (nullable = true)\n |-- mean_wind_speed: double (nullable = true)\n |-- num_mean_wind_speed_samples: long (nullable = true)\n |-- max_sustained_wind_speed: double (nullable = true)\n |-- max_gust_wind_speed: double (nullable = true)\n |-- max_temperature: double (nullable = true)\n |-- max_temperature_explicit: boolean (nullable = true)\n |-- min_temperature: double (nullable = true)\n |-- min_temperature_explicit: boolean (nullable = true)\n |-- total_precipitation: double (nullable = true)\n |-- snow_depth: double (nullable = true)\n |-- fog: boolean (nullable = true)\n |-- rain: boolean (nullable = true)\n |-- snow: boolean (nullable = true)\n |-- hail: boolean (nullable = true)\n |-- thunder: boolean (nullable = true)\n |-- tornado: boolean (nullable = true)\n\n"}], "source": "data.printSchema()"}, {"cell_type": "code", "execution_count": 7, "id": "a65dfb19-bc4f-4c30-a215-b5bb8a3953ef", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 5:>                                                          (0 + 4) / 4]\r"}, {"name": "stdout", "output_type": "stream", "text": "Number of rows is 114420316 and number of columns is 31\nCPU times: user 2.71 ms, sys: 2.83 ms, total: 5.54 ms\nWall time: 1.53 s\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "%%time\n## To look how many data points, we can use the command .count()\nprint(f\"Number of rows is {data.count()} and number of columns is {len(data.columns)}\")"}, {"cell_type": "markdown", "id": "1af63737-8238-4871-bf21-bbbeb2015d3b", "metadata": {}, "source": "114 million rows! Try to fit that into a Pandas DataFrame!. Now let's check how can we filter our dataframe and how can we create new columns.\n\nWe need to lever a very important set of Spark built-in functions from `pyspark.sql.functions`, typically called `F` functions"}, {"cell_type": "code", "execution_count": 27, "id": "203b6f4c-c628-4edc-af92-8d6d06f045ae", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- station_number: long (nullable = false)\n |-- wban_number: long (nullable = true)\n |-- year: long (nullable = false)\n |-- month: long (nullable = false)\n |-- day: long (nullable = false)\n |-- mean_temp: double (nullable = true)\n |-- num_mean_temp_samples: long (nullable = true)\n |-- mean_dew_point: double (nullable = true)\n |-- num_mean_dew_point_samples: long (nullable = true)\n |-- mean_sealevel_pressure: double (nullable = true)\n |-- num_mean_sealevel_pressure_samples: long (nullable = true)\n |-- mean_station_pressure: double (nullable = true)\n |-- num_mean_station_pressure_samples: long (nullable = true)\n |-- mean_visibility: double (nullable = true)\n |-- num_mean_visibility_samples: long (nullable = true)\n |-- mean_wind_speed: double (nullable = true)\n |-- num_mean_wind_speed_samples: long (nullable = true)\n |-- max_sustained_wind_speed: double (nullable = true)\n |-- max_gust_wind_speed: double (nullable = true)\n |-- max_temperature: double (nullable = true)\n |-- max_temperature_explicit: boolean (nullable = true)\n |-- min_temperature: double (nullable = true)\n |-- min_temperature_explicit: boolean (nullable = true)\n |-- total_precipitation: double (nullable = true)\n |-- snow_depth: double (nullable = true)\n |-- fog: boolean (nullable = true)\n |-- rain: boolean (nullable = true)\n |-- snow: boolean (nullable = true)\n |-- hail: boolean (nullable = true)\n |-- thunder: boolean (nullable = true)\n |-- tornado: boolean (nullable = true)\n |-- time: timestamp (nullable = true)\n\n"}], "source": "# Using built-in Spark functions are always more efficient\nfrom pyspark.sql import types\nimport pyspark.sql.functions as F\n\n## Let's create a new column called time\ndata = data.withColumn(\"time\", \n                  F.concat(F.col(\"year\"), \n                  F.lit(\"-\"), F.col(\"month\"), \n                  F.lit(\"-\"), F.col(\"day\")) \\\n                  .cast(types.TimestampType()))\n\ndata.printSchema()"}, {"cell_type": "code", "execution_count": null, "id": "c3088e15-208c-457c-af1d-05170165d34b", "metadata": {}, "outputs": [], "source": "# If you want to select one or a set of columns, we can use the select method\ndata.select('time').show(5)"}, {"cell_type": "code", "execution_count": null, "id": "8a7acff6-2368-411f-8002-85fab4a5dc2f", "metadata": {}, "outputs": [], "source": "# RUN CELL AS IS\ndata.select(['time', 'tornado']).show(5)"}, {"cell_type": "code", "execution_count": null, "id": "3b3ed2a7-90e9-442a-ad07-74cb0c42271d", "metadata": {}, "outputs": [], "source": "# If you want any row, we can take\ndata.take(1)"}, {"cell_type": "markdown", "id": "ab3baee7-17ed-4aed-b26f-651cd192e7e5", "metadata": {}, "source": "Each Row of the DataFrame is a `Row` which is similar to a dictionary, you can reference each element of the Row using the key. Now, also notice that the output of `take` is a list, so you need to index the list first"}, {"cell_type": "code", "execution_count": null, "id": "bf00c9ec-bfcb-418c-a25d-2c59e849313e", "metadata": {}, "outputs": [], "source": "# let's get the station_number only\ndata.take(1)[0]['station_number']"}, {"cell_type": "code", "execution_count": null, "id": "51d31ca6-be32-4e74-8b84-89bd2047bc1e", "metadata": {}, "outputs": [], "source": "# Let's check now how to filter data using another weather station data\nstations = spark.read.format('bigquery') \\\n  .option('table', 'bigquery-public-data:noaa_gsod.stations') \\\n  .load()"}, {"cell_type": "code", "execution_count": null, "id": "0009360a-86ea-48e6-96dd-4a239eea1abc", "metadata": {}, "outputs": [], "source": "## Let's filter only US based stations\n# Let's filter for just the US since this is a US based dataset\nstations_us = stations.filter(F.col('Country')=='US')\n\nprint(f'Total stations are {stations.count()}, total US stations are {stations_us.count()}')"}, {"cell_type": "code", "execution_count": null, "id": "3998e906-d562-4832-8c5b-e086b295c446", "metadata": {}, "outputs": [], "source": "%%time\n## Finally, we can describe our dataset using the describe command, similar to Pandas\n## Let's select just a few columns\nkeep_columns = ['station_number', 'mean_temp', 'thunder', 'mean_sealevel_pressure']\ndata.select(keep_columns).describe().show()"}, {"cell_type": "markdown", "id": "5eb0b10b-9c50-463d-b9ad-19b54b6baf02", "metadata": {}, "source": "# Data Types\n\nI highly recommend reading this article [Format Wars](http://www.svds.com/dataformats/) which covered the characteristics, structure, and differences between raw text, sequence, Avro, Parquet, and ORC data serializations. \n\nThere were several points discussed: \n\n* Human Readable\n* Row vs Column Oriented\n* Read vs Write performance\n* Appendable\n* Splittable\n* Metadata storage\n\nWe have 4 data types below\n\n- Compressed CSV\n- Parquet\n- Avro\n- CSV\n\nOf these 3 are row oriented and 1 is column oriented. We have over 100M rows and 31 columns. Columnar compression should do fairly well in this scenerio. "}, {"cell_type": "code", "execution_count": null, "id": "6610d716-b292-4a3f-a66c-62ca281a525d", "metadata": {}, "outputs": [], "source": "# Access staging bucket and see whats there\nimport os\nGCS_LOCATION = os.getenv('DATA_BUCKET')\nGCS_LOCATION"}, {"cell_type": "code", "execution_count": null, "id": "2432983d-7ace-47f6-9e4c-c129dec7001f", "metadata": {}, "outputs": [], "source": "%%time\n!gsutil rm -r {GCS_LOCATION}datagzip\ndata.write.option(\"compression\",\"gzip\").csv(f'{GCS_LOCATION}datagzip')\n!gsutil du -sh {GCS_LOCATION}datagzip/*"}, {"cell_type": "code", "execution_count": null, "id": "9dbe2d26-a679-48d5-9f2f-2d52eb86632a", "metadata": {}, "outputs": [], "source": "%%time\n!gsutil rm -r {GCS_LOCATION}dataparquet\ndata.write.format(\"parquet\").save(f'{GCS_LOCATION}dataparquet')\n!gsutil du -sh {GCS_LOCATION}dataparquet/*"}, {"cell_type": "code", "execution_count": null, "id": "81abdf35-320d-46f6-aea0-ee8f658d43dc", "metadata": {}, "outputs": [], "source": "%%time\n!gsutil rm -r {GCS_LOCATION}dataavro\ndata.write.format(\"avro\").save(f'{GCS_LOCATION}dataavro')\n!gsutil du -sh {GCS_LOCATION}dataavro/*"}, {"cell_type": "code", "execution_count": null, "id": "014d761f-3df7-4843-bf15-a5a01eb53562", "metadata": {}, "outputs": [], "source": "%%time\n!gsutil rm -r {GCS_LOCATION}datacsv\ndata.write.csv(f'{GCS_LOCATION}datacsv')\n!gsutil du -sh {GCS_LOCATION}datacsv/*"}, {"cell_type": "markdown", "id": "d5246998-fe8b-4ebc-9506-7805cf9620fc", "metadata": {}, "source": "## Why do we care?\n\nThe compression of each data type matter when running different operations and computations, let's compare the 3"}, {"cell_type": "code", "execution_count": 21, "id": "c040e61e-6195-422f-9d48-deba3b53e49b", "metadata": {}, "outputs": [], "source": "## Create our dataframes\ndata_parquet = spark.read.parquet(f'{GCS_LOCATION}dataparquet')\ndata_csv = spark.read.csv(f'{GCS_LOCATION}datacsv')\ndata_avro = spark.read.format(\"avro\").load(f'{GCS_LOCATION}/dataavro')"}, {"cell_type": "code", "execution_count": null, "id": "74327ef4-16f4-4575-bc14-144280b9aa24", "metadata": {}, "outputs": [], "source": "%%time\ndata_parquet.count()"}, {"cell_type": "code", "execution_count": null, "id": "5d30dd0b-a1d1-46a9-a24f-135840d396e4", "metadata": {}, "outputs": [], "source": "%%time\ndata_csv.count()"}, {"cell_type": "code", "execution_count": null, "id": "923c2adc-f5c9-4084-aa74-77179ecb554e", "metadata": {}, "outputs": [], "source": "%%time\ndata_avro.count()"}, {"cell_type": "markdown", "id": "3a206e12-2346-49fe-bf5b-f7105152e99c", "metadata": {}, "source": "* _What is the compression ratio for the parquet to csv file?_\n\n* _Which serialization would query a column faster?_\n\n* _Which types of columns do you think has the best compression for parquet?_\n\n* _When should you use flat files vs other data formats?_\n\n* _If we want to do analysis with lots of aggregations what serialization should we use?_\n\n* _Is there any downside to Parquet?_\n\n* _If you had to partition data into days as new data comes in with aggregations happening at end of day how would you operationalize this?_\n"}, {"cell_type": "markdown", "id": "d30f3717-7ca3-4fc7-aef8-93b103333410", "metadata": {}, "source": "# Data Aggregation\n\nLet's perform different aggregations using different methods and GroupBy. Don't worry! GroupBy from DataFrames is very different than RDDs."}, {"cell_type": "code", "execution_count": null, "id": "06cc5e36-e931-41d1-a678-1ad7ff98ef4d", "metadata": {}, "outputs": [], "source": "%%time\n## Let's start with sorting\n#data_parquet.sort(\"mean_temp\").show()\n#data_parquet.sort(\"mean_temp\").select(\"mean_temp\").show()\ndata_parquet.sort(\"mean_temp\").select(\"mean_temp\").filter(F.col(\"mean_temp\").isNotNull()).show()"}, {"cell_type": "code", "execution_count": null, "id": "4d5f9e54-3a19-4c5a-990f-d41a7463e64d", "metadata": {}, "outputs": [], "source": "%%time\n## Let's compare with avro\ndata_avro.sort(\"mean_temp\").select(\"mean_temp\").filter(F.col(\"mean_temp\").isNotNull()).show()"}, {"cell_type": "code", "execution_count": null, "id": "7e468f6e-4642-430b-9694-060ab2047bea", "metadata": {}, "outputs": [], "source": "%%time\ndata_parquet.select(F.mean(\"mean_wind_speed\").alias(\"Avg mean Wind\")).show()"}, {"cell_type": "code", "execution_count": null, "id": "ccde7268-495d-4cf1-a348-4f7a0d85b17b", "metadata": {}, "outputs": [], "source": "%%time\ndata_parquet.select(F.max(\"mean_wind_speed\")).show()"}, {"cell_type": "code", "execution_count": null, "id": "8ef360c5-e8a3-47ba-b230-d64d973b7fc2", "metadata": {}, "outputs": [], "source": "%%time\ndata_parquet.select(F.min(\"mean_wind_speed\")).show()"}, {"cell_type": "code", "execution_count": null, "id": "d5b1db71-4e91-46e9-a0ce-92595c9cfcf5", "metadata": {}, "outputs": [], "source": "%%time\ndata_parquet.select(F.stddev(\"mean_wind_speed\")).show()"}, {"cell_type": "markdown", "id": "0a10f8fe-4afd-4e67-96c4-b46498b2939d", "metadata": {}, "source": "## GroupBy"}, {"cell_type": "code", "execution_count": null, "id": "4542de49-597c-4fab-ac8f-06e91f462ad7", "metadata": {}, "outputs": [], "source": "%%time\n## Let's suppose we want the average temperature by year\ndata_pandas = data_parquet.groupBy(\"year\").agg(F.mean(\"mean_temp\").alias(\"Mean Temp\")).toPandas()\ndata_pandas.sort_values(\"year\").set_index(\"year\").plot()"}, {"cell_type": "code", "execution_count": null, "id": "263ce501-b7cc-4674-9924-08e6024b073b", "metadata": {}, "outputs": [], "source": "data_parquet.groupBy(\"year\").agg(F.mean(\"mean_temp\").alias(\"Mean Temp\")).count()"}, {"cell_type": "code", "execution_count": null, "id": "2aef51a6-92a2-40b3-a9b1-340eea20eb5c", "metadata": {}, "outputs": [], "source": "%%time\n## Let's suppose we want more than one\ndata_pandas = data_parquet.groupBy(\"year\").agg(F.mean(\"mean_temp\").alias(\"Mean Temp\"), \n                                               F.max(\"max_temperature\").alias(\"Max Temp\")).toPandas()\ndata_pandas.sort_values(\"year\").set_index(\"year\").plot()"}, {"cell_type": "code", "execution_count": null, "id": "b04f3f5f-73fd-4b29-b1d1-23f10508d2d2", "metadata": {}, "outputs": [], "source": "%%time\n## Let's suppose we want more than one\ndata_pandas = data_parquet.groupBy(\"year\").agg(F.mean(\"mean_temp\").alias(\"Mean Temp\"), \n                                               F.stddev(\"mean_temp\").alias(\"SD Mean Temp\")).toPandas()\ndata_pandas = data_pandas.sort_values(\"year\").set_index(\"year\")\ndata_pandas['Min CI Temp'] = data_pandas['Mean Temp'] - 2*data_pandas['SD Mean Temp']\ndata_pandas['Max CI Temp'] = data_pandas['Mean Temp'] + 2*data_pandas['SD Mean Temp']\ndel data_pandas['SD Mean Temp']\ndata_pandas.plot()"}, {"cell_type": "code", "execution_count": null, "id": "77120d21-d741-45ea-b264-0abe7b8665e2", "metadata": {}, "outputs": [], "source": "%%time\n## Let's suppose we want more than one\ndata_parquet.groupBy([\"year\", 'month']).agg(F.mean(\"mean_temp\").alias(\"Mean Temp\"), \n                                               F.stddev(\"mean_temp\").alias(\"SD Mean Temp\")).sort(['year','month']).show()"}, {"cell_type": "markdown", "id": "d31bf1ef-f3f1-45e1-b48d-af5b19806fbf", "metadata": {}, "source": "## User Defined Functions"}, {"cell_type": "code", "execution_count": 39, "id": "c9912b2e-eed1-4efd-81c9-28478ad9c240", "metadata": {}, "outputs": [], "source": "## Let's recall how we created the time column from before\ndata_parquet_time = data_parquet.withColumn(\"time\", \n                                F.concat(F.col(\"year\"), \n                                F.lit(\"-\"), F.col(\"month\"), \n                                F.lit(\"-\"), F.col(\"day\")) \\\n                                .cast(types.TimestampType()))"}, {"cell_type": "code", "execution_count": null, "id": "7e7229b7-a1c0-46fb-95dd-569a09ecaa6f", "metadata": {}, "outputs": [], "source": "%%time\ndata_parquet_time.select('time').show(5)"}, {"cell_type": "code", "execution_count": 41, "id": "26a745e0-ad2d-4497-8dc8-4a34aaaf1122", "metadata": {}, "outputs": [], "source": "## Can we do it differently? Yes! UDF. You can create UDF that will work row by row in your dataframe\ndef create_date_from_parts(year, month, day):\n    return f'{year}-{month}-{day}'\n\ncreate_date_udf = F.udf(create_date_from_parts, types.StringType())\ndata_parquet_time_udf = data_parquet.withColumn(\"time\", create_date_udf('year', 'month', 'day').cast(types.TimestampType()))"}, {"cell_type": "code", "execution_count": null, "id": "6d654734-5240-4063-9a24-8fa5105a28a0", "metadata": {}, "outputs": [], "source": "%%time\ndata_parquet_time_udf.select('time').show(5)"}, {"cell_type": "markdown", "id": "1d3c75eb-de19-41f4-91f0-087fe76816e4", "metadata": {}, "source": "UDFs are typically much slower than built-in Spark functionality. The reason for this is becauase they have to serialize and deserialize the data for every row that the function is applied to. There have been recent improvements to UDF for some analytical results with Pandas UDFs that return scalars or groupby maps. Some more information about why UDFs are inefficent can be found here https://blog.cloudera.com/blog/2017/02/working-with-udfs-in-apache-spark/"}, {"cell_type": "code", "execution_count": null, "id": "8b6d479f-9ca4-46b8-848d-72e3a922330a", "metadata": {}, "outputs": [], "source": "%%time\n## Let's look at other examples\nfrom pyspark.sql.functions import udf\n@udf(\"double\")\ndef squared_udf(s):\n    return s * s\n\n\ndata_udf = data_parquet.withColumn(\"square_temp\", squared_udf(F.col(\"mean_temp\")))\ndata_udf.select(\"square_temp\").show()"}, {"cell_type": "code", "execution_count": null, "id": "fc737537-8942-4ecd-92d6-694e4b37dd9a", "metadata": {}, "outputs": [], "source": "%%time\ndata_no_udf = data_parquet.withColumn(\"square_temp\", F.col(\"mean_temp\")**2)\ndata_udf.select(\"square_temp\").show()"}, {"cell_type": "code", "execution_count": null, "id": "7304527e-7f4f-41e2-961d-2e20df225b18", "metadata": {}, "outputs": [], "source": "%%time\n## You can also use UDF with select\ndata_parquet.select(\"mean_temp\", squared_udf(\"mean_temp\").alias(\"squared_temp\")).show()"}, {"cell_type": "markdown", "id": "b33a2815-a1c3-47ef-9e94-ddcb31373f64", "metadata": {}, "source": "# Spark SQL\n\nFinally, let's work with Spark SQL. Spark allows us to combine the power of SQL with Spark and the Dataframes API"}, {"cell_type": "code", "execution_count": null, "id": "718fa0fd-d510-41e7-879f-b39e1bb89854", "metadata": {}, "outputs": [], "source": "%%time\n## Let's run an example\n# First we need to create a temporary table that we can query\ndata_parquet.registerTempTable('data')\nspark.sql(\n\"\"\"\nselect mean_temp\nfrom data\n\"\"\").show()"}, {"cell_type": "code", "execution_count": null, "id": "60ea89c4-3253-49e5-a6b7-42f65ce3c9a5", "metadata": {}, "outputs": [], "source": "%%time\ndata_parquet.select(\"mean_temp\").show()"}, {"cell_type": "code", "execution_count": null, "id": "fc3c44e8-1f1b-4a9b-8852-a4b7e907a813", "metadata": {}, "outputs": [], "source": "## Let's run multiple querys similar to the ones we ran before\nspark.sql(\n\"\"\"\nselect mean_temp, power(mean_temp, 2) as squared_temp\nfrom data\n\"\"\").show()"}, {"cell_type": "code", "execution_count": null, "id": "52685783-f794-4c77-bc1a-cd847762d081", "metadata": {}, "outputs": [], "source": "%%time\n## Let's run multiple querys similar to the ones we ran before\nspark.sql(\n\"\"\"\nselect \n    year, \n    month, \n    avg(mean_temp) as mean, \n    std(mean_temp) as st_dev\nfrom \n    data\ngroup by\n    year,\n    month\norder by\n    year,\n    month\n\"\"\").show()"}, {"cell_type": "code", "execution_count": null, "id": "cd675f01-2aa0-45d7-b499-16e03b2920dc", "metadata": {}, "outputs": [], "source": "## We can save the Spark SQL query as a dataframe\ndf_sql = spark.sql(\n\"\"\"\nselect \n    year, \n    month, \n    avg(mean_temp) as mean, \n    std(mean_temp) as st_dev\nfrom \n    data\ngroup by\n    year,\n    month\norder by\n    year,\n    month\n\"\"\")\n\ndf_sql_pd = df_sql.toPandas()\ndf_sql_pd = df_sql_pd.set_index([\"year\", 'month'])\ndf_sql_pd"}, {"cell_type": "code", "execution_count": null, "id": "72010449-da7d-4f4b-9133-799e7f36fea7", "metadata": {}, "outputs": [], "source": "%%time\n## Let's also join dataframes\nstations = spark.read.format('bigquery') \\\n  .option('table', 'bigquery-public-data:noaa_gsod.stations') \\\n  .load()\n\nstations_us = stations.filter(F.col('Country')=='US')\n\n## One of the dataframes is quite small, so let's broadcast it!\n# join_data = data_parquet.join(F.broadcast(stations_us), stations_us.usaf==data_parquet.station_number, 'inner')\njoin_data = data_parquet.join(stations_us, stations_us.usaf==data_parquet.station_number, 'inner')\njoin_data.count()"}, {"cell_type": "code", "execution_count": 53, "id": "8576a358-9a89-4000-8d86-9a0b303d84f7", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- station_number: long (nullable = true)\n |-- wban_number: long (nullable = true)\n |-- year: long (nullable = true)\n |-- month: long (nullable = true)\n |-- day: long (nullable = true)\n |-- mean_temp: double (nullable = true)\n |-- num_mean_temp_samples: long (nullable = true)\n |-- mean_dew_point: double (nullable = true)\n |-- num_mean_dew_point_samples: long (nullable = true)\n |-- mean_sealevel_pressure: double (nullable = true)\n |-- num_mean_sealevel_pressure_samples: long (nullable = true)\n |-- mean_station_pressure: double (nullable = true)\n |-- num_mean_station_pressure_samples: long (nullable = true)\n |-- mean_visibility: double (nullable = true)\n |-- num_mean_visibility_samples: long (nullable = true)\n |-- mean_wind_speed: double (nullable = true)\n |-- num_mean_wind_speed_samples: long (nullable = true)\n |-- max_sustained_wind_speed: double (nullable = true)\n |-- max_gust_wind_speed: double (nullable = true)\n |-- max_temperature: double (nullable = true)\n |-- max_temperature_explicit: boolean (nullable = true)\n |-- min_temperature: double (nullable = true)\n |-- min_temperature_explicit: boolean (nullable = true)\n |-- total_precipitation: double (nullable = true)\n |-- snow_depth: double (nullable = true)\n |-- fog: boolean (nullable = true)\n |-- rain: boolean (nullable = true)\n |-- snow: boolean (nullable = true)\n |-- hail: boolean (nullable = true)\n |-- thunder: boolean (nullable = true)\n |-- tornado: boolean (nullable = true)\n |-- usaf: string (nullable = true)\n |-- wban: string (nullable = true)\n |-- name: string (nullable = true)\n |-- country: string (nullable = true)\n |-- state: string (nullable = true)\n |-- call: string (nullable = true)\n |-- lat: double (nullable = true)\n |-- lon: double (nullable = true)\n |-- elev: string (nullable = true)\n |-- begin: string (nullable = true)\n |-- end: string (nullable = true)\n\n"}], "source": "join_data.printSchema()"}, {"cell_type": "code", "execution_count": null, "id": "11519ef3-e006-4462-af89-948da187441b", "metadata": {}, "outputs": [], "source": "%%time\n## let's now use Spark SQL to query this data\njoin_data.registerTempTable('data')\nspark.sql(\n\"\"\"\nselect \n    state,\n    avg(mean_temp) as mean, \n    avg(lat),\n    avg(lon)\nfrom \n    data\nwhere\n    state in ('CA', 'TX', 'NY')\ngroup by\n    state\norder by\n    state\n\"\"\").show()"}, {"cell_type": "code", "execution_count": null, "id": "dee58003-63d6-47c3-b4ad-c6d44cc2529a", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.13"}}, "nbformat": 4, "nbformat_minor": 5}