{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# wk4 Demo - Intro to Spark\n__`MIDS w261: Machine Learning at Scale | UC Berkeley School of Information | Fall 2022`__\n\nLast week we saw a number of design patterns in Hadoop MapReduce. This week we will look at the limitations of Hadoop MapReduce when it comes to running iterative jobs and preview the advantages of modern distributed compuation frameworks like Spark. By abstracting away many of the parallelization details Spark provides a flexible interface for the programmer. However a word of warning: don't let the ease of implementation lull you into complacency, scalable solutions still require attention to the details of smart algorithm design. \n\nIn class today we'll get some practice working with Spark RDDS. We'll use Spark to re-implement each of the tasks that you performed using the Command Line or Hadoop Streaming in weeks 1-3 of the course. Our goal is to get you up to speed and coding in Spark as quickly as possible; this is by no means a comprehensive tutorial. By the end of today's demo you should be able to:  \n* ... __initialize__ a `SparkSession` in a local NB and use it to run a Spark Job.\n* ... __access__ the Spark Job Tracker UI.\n* ... __describe__ and __create__ RDDs from files or local Python objects.\n* ... __explain__ the difference between actions and transformations.\n* ... __decide__ when to `cache` or `broadcast` part of your data.\n* ... __implement__ Word Counting, Sorting and Naive Bayes in Spark. \n\n__`NOTE:`__ Although RDD successor datatype, Spark dataframes, are becoming more common in production settings we've made a deliberate choice to teach you RDDs first beause building homegrown algorithm implementations is crucial to developing a deep understanding of machine learning and parallelization concepts -- which is the goal of this course. We'll still touch on dataframes in Week 5 when talking about Spark efficiency considerations and we'll do a deep dive into Spark dataframes and streaming solutions in Week 12.\n\n__`Additional Resources:`__ The offical documentation pages offer a user friendly overview of the material covered in this week's readings: [Spark RDD Programming Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-programming-guide).\n\n__RDD API docs__: https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD"}, {"cell_type": "markdown", "metadata": {}, "source": "### Notebook Set-Up"}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "/media/main/LiveSessionMaterials/wk04Demo_IntroToSpark\n"}], "source": "# RUN CELL AS IS\nimport os\nDATA_BUCKET = os.getenv('DATA_BUCKET','')[:-1] # our private storage bucket location\nDEMO04_FOLDER = f\"{DATA_BUCKET}/notebooks/jupyter/LiveSessionMaterials/wk04Demo_IntroToSpark\"\nprint(f\"Personal Data bucket:  {DATA_BUCKET}\")\n!gsutil ls -lh  {DEMO04_FOLDER}  "}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": "# imports\nimport re\nimport numpy as np\nimport pandas as pd"}, {"cell_type": "markdown", "metadata": {}, "source": "### Load the data\nToday we'll mostly be working with toy examples & data created on the fly in Python. However at the end of this demo we'll revisit Word Count & Naive Bayes using some of the data from weeks 1-3. Run the following cells to re-load the _Alice in Wonderland_ text & the 'Chinese' toy example."}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Copying gs://w261-hw-data/main/Assignments/HW1/data/alice.txt...\n/ [1 files][170.2 KiB/170.2 KiB]                                                \nOperation completed over 1 objects/170.2 KiB.                                    \n"}], "source": "# (re)download alice.txt used in HW1\n!gsutil cp gs://w261-hw-data/main/Assignments/HW1/data/alice.txt {DEMO04_FOLDER}/data/alice.txt\nALICE_TXT = DEMO04_FOLDER + \"/data/alice.txt\""}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Writing data/chineseTrain.txt\n"}], "source": "#%%writefile data/chineseTrain.txt\nchinese_train = '''D1\t1\t\tChinese Beijing Chinese\nD2\t1\t\tChinese Chinese Shanghai\nD3\t1\t\tChinese Macao\nD4\t0\t\tTokyo Japan Chinese'''\n\n!echo \"{chinese_train}\" | gsutil cp - {DEMO04_FOLDER}/data/chineseTrain.txt"}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Writing data/chineseTest.txt\n"}], "source": "#%%writefile data/chineseTest.txt\nchinese_test = '''D5\t1\t\tChinese Chinese Chinese Tokyo Japan\nD6\t1\t\tBeijing Shanghai Trade\nD7\t0\t\tJapan Macao Tokyo\nD8\t0\t\tTokyo Japan Trade'''\n\n!echo \"{chinese_test}\" | gsutil cp - {DEMO04_FOLDER}/data/chineseTest.txt"}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [], "source": "# naive bayes toy example data paths - ADJUST AS NEEDED\nTRAIN_PATH = DEMO04_FOLDER + \"/data/chineseTrain.txt\"\nTEST_PATH = DEMO04_FOLDER + \"/data/chineseTest.txt\""}, {"cell_type": "markdown", "metadata": {}, "source": "# Exercise 1. Getting started with Spark. \nFor week 4 you read Ch 3-4 from _Learning Spark: Lightning-Fast Big Data Analysis_ by Karau et. al. as well as a few blog posts that set the stage for Spark. From these readings you should be familiar with each of the following terms:\n\n* __Spark session__\n* __Spark context__\n* __driver program__\n* __executor nodes__\n* __resilient distributed datasets (RDDs)__\n* __pair RDDs__\n* __actions__ and __transformations__\n* __lazy evaluation__\n\nThe first code block below shows you how to start a `SparkSession` in a Jupyter Notebook. Next we show a simple example of creating and transforming a Spark RDD. Let's use this as a quick vocab review before we dive into more interesting examples. "}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n22/05/24 19:47:17 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n22/05/24 19:47:17 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n22/05/24 19:47:17 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n22/05/24 19:47:17 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n"}], "source": "from pyspark.sql import SparkSession\napp_name = \"wk4_demo\"\nmaster = \"local[*]\"\nspark = SparkSession\\\n        .builder\\\n        .appName(app_name)\\\n        .master(master)\\\n        .config(\"spark.ui.port\",\"42229\")\\\n        .getOrCreate()\nsc = spark.sparkContext"}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [], "source": "# a small example\nmyData = sc.parallelize(range(1,100))\nsquares = myData.map(lambda x: (x,x**2))\noddSquares = squares.filter(lambda x: x[1] % 2 == 1)"}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "[(1, 1), (3, 9), (5, 25), (7, 49), (9, 81)]"}, "execution_count": 17, "metadata": {}, "output_type": "execute_result"}], "source": "oddSquares.take(5)"}, {"cell_type": "markdown", "metadata": {}, "source": " > __DISCUSSION QUESTIONS:__ For each key term from the reading, briefly explain what it means in the context of this demo code. Specifically:\n * _What is the 'driver program' here?_\n * _What does the spark context do? Do we have 'executors' per se?_\n * _List all RDDs and pair RDDs present in this example._\n * _List all transformations present in this example._\n * _List all actions present in this example._\n * _What does the concept of 'lazy evaluation' mean about the time it would take to run each cell in the example?_\n * _If we were working on a cluster, where would each transformation happen? would the data get shuffled?_"}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": "# from IPython.display import Image \n# Image(filename=\"spark-context.png\")"}, {"cell_type": "markdown", "metadata": {}, "source": "* List all RDDs and pair RDDs present in this example.\n> RDDs: `myData`; Pair RDDs: `squares` and `oddSquares`\n\n\n* List all transformations present in this example.\n> `map()` and `filter()`\n\n* List all actions present in this example.\n> `take()`\n\n* What does the concept of 'lazy evaluation' mean about the time it would take to run each cell in the example?\n> Lazy evaluation means that Spark keeps track of a directed acyclic graph (DAG) that contains the list of transformations needed to recreate each RDD and only actually performs those transformations when an action is called. This means that when we run the cell with 3 lines of code, no transformations actually happen, whereas running the `take` action triggers the computation to start.\n\n* If we were working on a cluster, where would each transformation happen? would the data get shuffled?\n> There are two transformations in this code: map and filter. Both will happen on the nodes where the data are located. The data will not be shuffled in this code."}, {"cell_type": "markdown", "metadata": {}, "source": "# Exercise 2. RDD transformations warm ups.\n\nHere are some more examples of Spark transformations and actions. For each task below, we've provided a few different implementations. Read each example and discuss the differences. Is one implementation better than the other or are the differences cosmetic? You may wish to discuss:\n* the format of the data after each transformation\n* memory usage (on executor nodes & in the driver)\n* time complexity\n* amount of network transfer\n* whether or not the data will get shuffled\n* coding efficiency & readability  \n\nAlthough we're working with tiny demo examples for now, try to imagine how the same code would operate if we were running a large job on a cluster. To aid in your analysis, navigate to the Spark UI (available at http://localhost:4040). To start, you should see a single job -- the job from Exercise 1. Click on the job description to view the DAG for that job. Check back with this UI as you run each version of the tasks below (__Note__: _the stages tab may be particularly helpful when making your comparisons_).\n\n#### a) Multiples of 5 and 7"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# VERSION 1\ndataRDD = sc.parallelize(range(1,100))\nfivesRDD = dataRDD.filter(lambda x: x % 5 == 0)\nsevensRDD = dataRDD.filter(lambda x: x % 7 == 0)\nresult = fivesRDD.intersection(sevensRDD)\nresult.collect()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# VERSION 2\ndataRDD = sc.parallelize(range(1,100))\nresult = dataRDD.filter(lambda x: x % 5 == 0)\\\n                .filter(lambda x: x % 7 == 0)\nresult.collect()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# VERSION 3\ndataRDD = sc.parallelize(range(1,100))\nresult = dataRDD.filter(lambda x: x % 7 == 0 and x % 5 == 0)\nresult.collect()"}, {"cell_type": "markdown", "metadata": {}, "source": ">__DISCUSSION QUESTION:__ \n* What is the task here? Compare/contrast these three implementations.  \n* Which of these versions require a shuffle? How do you know?"}, {"cell_type": "markdown", "metadata": {}, "source": "#### b) Pig Latin Translator"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "poem = [\"A bear however hard he tries\", \n        \"Grows tubby without exercise\", \n        \"said AA Milne\"]"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# VERSION 1\ndef translate(sent):\n    words = [w[1:] + w[0] + '-ay' for w in sent.lower().split()]\n    return ' '.join(words)\n\npoemRDD = sc.parallelize(poem)\nresult = poemRDD.map(translate)\\\n                .reduce(lambda x,y: x + ' ' + y)\nprint(result)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# VERSION 2\ndef translate(wrd):\n    return wrd[1:] + wrd[0] + '-ay'\n\npoemRDD = sc.parallelize(poem)\nresult = poemRDD.flatMap(lambda x: x.lower().split())\\\n                .map(translate)\\\n                .reduce(lambda x,y: x + ' ' + y)\nprint(result)"}, {"cell_type": "markdown", "metadata": {}, "source": ">__DISCUSSION QUESTION:__ What is the task here? Compare/contrast these two implementations."}, {"cell_type": "markdown", "metadata": {}, "source": "#### c) Average Monthly Purchases"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "shoppingList = [\"JAN: 5 apples, 15 oranges\",\n                \"FEB: 10 apples, 10 oranges\",\n                \"MAR: 3 apples, 1 oranges\",\n                \"APR: 6 apples, 2 oranges\"]"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# helper function\ndef parseShopping(line):\n    \"\"\"Parse each month's shopping list string into a key-value iterator.\"\"\"\n    month, items = line.split(':')\n    items = [item.strip().split(' ') for item in items.split(',')]\n    return [(i[1], int(i[0])) for i in items]"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# VERSION 1  (example 4-7 from Learning Spark)\nshoppingRDD = sc.parallelize(shoppingList)\nresult = shoppingRDD.flatMap(lambda x: parseShopping(x))\\\n                    .mapValues(lambda x: (x,1))\\\n                    .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\\\n                    .mapValues(lambda x: x[0]/float(x[1]))\nresult.collect()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# VERSION 2 (example 4-12 from Learning Spark)\nshoppingRDD = sc.parallelize(shoppingList)\nresult = shoppingRDD.flatMap(lambda x: parseShopping(x))\\\n                    .combineByKey(lambda x: (x,1),       # action for new key\n                                  lambda x, y: (x[0] + y, x[1] + 1), # action for repeat key\n                                  lambda x, y: (x[0] + y[0], x[1] + y[1]))\\\n                    .mapValues(lambda x: x[0]/float(x[1]))\nresult.collect()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# VERSION 3\nshoppingRDD = sc.parallelize(shoppingList)\nresult = shoppingRDD.flatMap(lambda x: parseShopping(x))\\\n                    .groupByKey()\\\n                    .mapValues(lambda x: sum(x)/float(len(x)))\nresult.collect()"}, {"cell_type": "markdown", "metadata": {}, "source": ">__DISCUSSION QUESTION:__ What is the task here? Compare/contrast these three implementations."}, {"cell_type": "markdown", "metadata": {}, "source": "#  Exercise 3. cache()-ing\n\nIn exercise 2 you saw how Spark builds an execution plan (DAG) so that transformations are evaluated lazily when triggerd by an action. In more complex DAGs you may need to reuse the contents of an RDD for multiple downstream operations. In such cases we'd like to avoid duplicating the computation of that intermediate result. Spark offers a few different options to persist an RDD in memory on the executor node where it is stored. Of these the most common is `cache()` (you'll read about others next week in ch 5 from _High Performance Spark_). Lets briefly look at how to `cache()` an RDD and discus when doing so is to your advantage."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# initialize data\ndataRDD = sc.parallelize(np.random.random_sample(1000))   "}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# perform some transformations\ndata2X= dataRDD.map(lambda x: x*2)\ndataGreaterThan1 = data2X.filter(lambda x: x > 1.0)\ncachedRDD = dataGreaterThan1.cache()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# count results less than 1\ncachedRDD.filter(lambda x: x<1).count()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# count results greater than 1\ncachedRDD.filter(lambda x: x>1).count()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# look at 10 results\nfor line in cachedRDD.take(10):\n    print(line)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# look at top 10 results\nfor line in cachedRDD.top(10):\n    print(line)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# look at top 10 results\nfor line in cachedRDD.takeOrdered(10):\n    print(line)"}, {"cell_type": "markdown", "metadata": {}, "source": "> __DISCUSSION QUESTIONS:__\n* How many total actions are there in the 7 cells above?\n* If we hadn't cached the `dataGreaterThan1` RDD what would happen each time we call an action?\n* How does `cache()` change what the framework does? \n* When does it _not_ make sense to `cache()` an intermediate result?"}, {"cell_type": "markdown", "metadata": {}, "source": "# Exercise 4. broadcast()-ing\n\nAnother challenge we faced when designing Hadoop MapReduce jobs was the challenge of making key pieces of information available to multiple nodes so that certain computations can happen in parallel. In Hadoop Streaming we resolved this challenge using custom partition keys and the order inversion pattern. In Spark we'll use broadcast variables -- read only objects that Spark will ship to all nodes where they're needed. Here's a brief example of how to create and access a broadcast variable."}, {"cell_type": "markdown", "metadata": {}, "source": "Run the following cell to create our sample data files: a list of customers & a list of cities."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#%%writefile data/customers.csv\ncustomers = '''Quinn Frank,94703\nMorris Hardy,19875\nTara Smith,12204\nSeth Mitchell,38655\nFinley Cowell,10005\nCory Townsend,94703\nMira Vine,94016\nLea Green,70118\nV Neeman,16604\nTvei Qin,70118'''\n\n!echo \"{customers}\" | gsutil cp - {DEMO04_FOLDER}/data/customers.csv"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#%%writefile data/zipCodes.csv\nzip_codes = '''94703,Berkeley,CA\n94016,San Francisco,CA\n10005,New York,NY\n12204,Albany,NY\n38655,Oxford,MS\n70118,New Orleans,LA'''\n\n!echo \"{zip_codes}\" | gsutil cp - {DEMO04_FOLDER}/data/zipCodes.csv"}, {"cell_type": "markdown", "metadata": {}, "source": "Spark Job to count customers by state."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# load customers into RDD\ndataRDD = sc.textFile(f'{DEMO04_FOLDER}/data/customers.csv')"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# create a look up dictionary to map zip codes to state abbreviations\nzipCodes = sc.textFile(f'{DEMO04_FOLDER}/data/zipCodes.csv')\\\n             .map(lambda l: tuple(l.split(',')))\\\n             .collect()\nzipCodes = {item[0]: f'{item[2]}' for item in zipCodes}\n\nzipCodes = sc.broadcast(zipCodes)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# count by state\nresult = dataRDD.map(lambda x: x.split(',')[1])\\\n                .map(lambda x: (zipCodes.value.get(x,'n/a'),1))\\\n                .reduceByKey(lambda a, b: a + b)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# take a look\nresult.collect()"}, {"cell_type": "markdown", "metadata": {}, "source": "> __DISCUSSION QUESTIONS:__\n* What does broadcasting achieve here?\n* Why not just encapsulate our variables in a function closure instead?\n* When would it be a bad idea to broadcast a supplemental table like our list of zip codes?\n* Note that we are working in local mode through out this notebook. What happens if you comment out the line where we broadcast the zip code dictionary? What would happen if you were working on a cluster?"}, {"cell_type": "markdown", "metadata": {}, "source": "# Exercise 5. Accumulators\n\nAccumulators are Spark's equivalent of Hadoop counters. Like broadcast variables they represent shared information across the nodes in your cluster, but unlike broadcast variables accumulators are _write-only_ ... in other words you can only access their values in the driver program and not on your executors (where transformations are applied). As convenient as this sounds, there are a few common pitfalls to avoid. Let's take a look.\n\nRun the following cell to create a sample data file representing a list of `studentID, courseID, final_grade`..."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#%%writefile data/grades.csv\ngrades = '''10001,101,98\n10001,102,87\n10002,101,75\n10002,102,55\n10002,103,80\n10003,102,45\n10003,103,75\n10004,101,90\n10005,101,85\n10005,103,60'''\n\n!echo \"{grades}\" | gsutil cp - {DEMO04_FOLDER}/data/grades.csv"}, {"cell_type": "markdown", "metadata": {}, "source": "Suppose we want to compute the average grade by course and student while also tracking the number of failing grades awarded. We might try something like this:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# initialize an accumulator to track failing grades\nnFailing = sc.accumulator(0)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# function to increment the accumulator as we read in the data\ndef parse_grades(line, accumulator):\n    \"\"\"Helper function to parse input & track failing grades.\"\"\"\n    student,course,grade = line.split(',')\n    grade = int(grade)\n    if grade < 65:\n        accumulator.add(1)\n    return(student,course, grade)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# compute averages in spark\ngradesRDD = sc.textFile(f'{DEMO04_FOLDER}/data/grades.csv')\\\n              .map(lambda x: parse_grades(x, nFailing))\nstudentAvgs = gradesRDD.map(lambda x: (x[0], (x[2], 1)))\\\n                       .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\\\n                       .mapValues(lambda x: x[0]/x[1])\ncourseAvgs = gradesRDD.map(lambda x: (x[1], (x[2], 1)))\\\n                      .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\\\n                      .mapValues(lambda x: x[0]/x[1])"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# take a look\nprint(\"===== average by student =====\")\nprint(studentAvgs.collect())\nprint(\"===== average by course =====\")\nprint(courseAvgs.collect())\nprint(\"===== number of failing grades awarded =====\")\nprint(nFailing)"}, {"cell_type": "markdown", "metadata": {}, "source": "> __DISCUSSION QUESTIONS:__\n* What is wrong with the results? (__`HINT:`__ _how many failing grades are there really?_)\n* Why might this be happening? (__`HINT:`__ _How many actions are there in this code? Which parts of the DAG are recomputed for each of these actions?_)\n* What one line could we add to the code to fix this problem?\n  * What could go wrong with our \"fix\"?\n* How could we have designed our parser differently to avoid this problem in the first place?"}, {"cell_type": "markdown", "metadata": {}, "source": "# Exercise 6. WordCount & Naive Bayes Reprise\n\nWe'll wrap up today's demo by revisiting two tasks from weeks 1-2. Compare each of these Spark implementations to the approach we took when performing the same task in Hadoop MapReduce."}, {"cell_type": "markdown", "metadata": {}, "source": "### a) Word Count in Spark"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# load the data into Spark\naliceRDD = sc.textFile(ALICE_TXT)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# perform wordcount\nresult = aliceRDD.flatMap(lambda line: re.findall('[a-z]+', line.lower())) \\\n                 .map(lambda word: (word, 1)) \\\n                 .reduceByKey(lambda a, b: a + b)\\\n                 .cache()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# take a look at the top 10 (by alphabet)\nresult.takeOrdered(10)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# take a look at the top 10 (by count)\nresult.takeOrdered(10, key=lambda x: -x[1])"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# what does Spark consider the 'top'?\nresult.top(10)"}, {"cell_type": "markdown", "metadata": {}, "source": "> __DICUSSION QUESTIONS:__\n* Compare/contrast this implementation to our Hadoop Streaming approach.\n* How many times does the data get shuffled?\n* What local aggregation will spark do?\n* What is the difference between `take()` and `top()` and `takeOrdered()`? Is one more or less efficient than the others? Compare these actions to the work we had to do to sort and subset with multiple reducers in Hadoop MapReduce?\n* What would happen if we removed the `cache()` that follows the `reduceByKey()`? [__`Hint:`__ _this is kind of a trick question, but try rerunning the job & look at the Spark UI..._]"}, {"cell_type": "markdown", "metadata": {}, "source": "### b) Naive Bayes in Spark\nImplement the algorithm (training & inference)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "def parse(doc):\n    \"\"\"\n    Helper Function to parse documents.\n    \"\"\"\n    docID, class_, subj, body = doc.lower().split('\\t')\n    return(class_, subj + \" \" + body)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "def tokenize(class_, text):\n    \"\"\"\n    Map text from a given class to word list with counts for each class. \n    \"\"\"\n    # get words                       \n    words = re.findall(r'[a-z]+', text)\n    # emit a count for each class (0,1 or 1,0)\n    class_counts = [1,0] if class_ =='0' else [0,1]\n    return[(word, class_counts) for word in words]"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "def NBtrain(dataRDD, smoothing = 1.0):\n    \"\"\"\n    Function to train a Naive Bayes Model in Spark.\n    Returns a dictionary.\n    \"\"\"\n    # extract word counts\n    docsRDD = dataRDD.map(parse)\n    wordsRDD = docsRDD.flatMap(lambda x: tokenize(*x)).cache()\\\n                      .reduceByKey(lambda x,y: np.array(x) + np.array(y))\\\n                      .cache()\n    # compute priors\n    docTotals = docsRDD.countByKey()\n    priors = np.array([docTotals['0'], docTotals['1']])\n    priors = priors/sum(priors)\n    \n    # compute conditionals\n    wordTotals = sc.broadcast(wordsRDD.map(lambda x: x[1] + np.array([smoothing, smoothing]))\\\n                                      .reduce(lambda x,y: np.array(x) + np.array(y)))\n    cProb = wordsRDD.mapValues(lambda x: x + np.array([smoothing, smoothing]))\\\n                    .mapValues(lambda x: x/np.array(wordTotals.value))\\\n                    .collect()\n    \n    return dict([(\"ClassPriors\", priors)] + cProb)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "def NBclassify(document, model_dict):\n    \"\"\"\n    Classify a document as ham/spam via Naive Bayes.\n    Use logProbabilities to avoid floating point error.\n    NOTE: this is just a python function, no distribution so \n    we should expect our documents (& model) to fit in memory.\n    \"\"\"\n    # get words                       \n    words = re.findall(r'[a-z]+', document.lower())\n    # compute log probabilities\n    logProbs = [np.log(model_dict.get(wrd,[1,1])) for wrd in words]\n    # return most likely class\n    sumLogProbs = np.log(model_dict['ClassPriors']) + sum(logProbs)\n    return np.argmax(sumLogProbs)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "def evaluate(resultsRDD):\n    \"\"\"\n    Compute accuracy, precision, recall an F1 score given a\n    pairRDD of (true_class, predicted_class)\n    \"\"\"\n    nDocs = resultsRDD.count()\n    TP = resultsRDD.filter(lambda x: x[0] == '1' and x[1] == 1).count()\n    TN = resultsRDD.filter(lambda x: x[0] == '0' and x[1] == 0).count()\n    FP = resultsRDD.filter(lambda x: x[0] == '0' and x[1] == 1).count()\n    FN = resultsRDD.filter(lambda x: x[0] == '1' and x[1] == 0).count()\n    \n    # report results \n    print(f\"Total # Documents:\\t{nDocs}\")\n    print(f\"True Positives:\\t{TP}\") \n    print(f\"True Negatives:\\t{TN}\")\n    print(f\"False Positives:\\t{FP}\")\n    print(f\"False Negatives:\\t{FN}\") \n    print(f\"Accuracy\\t{(TP + TN)/(TP + TN + FP + FN)}\")\n    if (TP + FP) != 0:  \n        precision = TP / (TP + FP)  \n        print(f\"Precision\\t{precision}\")\n    if (TP + FN) != 0: \n        recall = TP / (TP + FN) \n        print(f\"Recall\\t{recall}\") \n    if TP != 0: \n        f_score = 2 * precision * recall / (precision + recall)\n        print(f\"F-Score\\t{f_score}\")"}, {"cell_type": "markdown", "metadata": {}, "source": "Retrieve results."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# load data into Spark\ntrainRDD = sc.textFile(TRAIN_PATH)\ntestRDD = sc.textFile(TEST_PATH)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# train your model (& take a look)\nNBmodel = NBtrain(trainRDD)\nNBmodel"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# perform inference on a doc (just to test)\nNBclassify(\"This Japan Tokyo Macao is Chinese\", NBmodel)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# evaluate your model\nmodel_b = sc.broadcast(NBmodel)\nresultsRDD = testRDD.map(parse)\\\n                    .mapValues(lambda x: NBclassify(x, model_b.value))\nevaluate(resultsRDD)"}, {"cell_type": "markdown", "metadata": {}, "source": "> __DICUSSION QUESTIONS:__\n* Compare/contrast this implementation to our Hadoop Streaming approach."}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.13"}, "toc": {"colors": {"hover_highlight": "#DAA520", "navigate_num": "#000000", "navigate_text": "#333333", "running_highlight": "#FF0000", "selected_highlight": "#FFD700", "sidebar_border": "#EEEEEE", "wrapper_background": "#FFFFFF"}, "moveMenuLeft": true, "nav_menu": {"height": "462px", "width": "252px"}, "navigate_menu": true, "number_sections": true, "sideBar": true, "threshold": 4, "toc_cell": true, "toc_position": {"height": "567px", "left": "0px", "right": "707.4456787109375px", "top": "105px", "width": "243px"}, "toc_section_display": "block", "toc_window_display": true, "widenNotebook": false}}, "nbformat": 4, "nbformat_minor": 4}